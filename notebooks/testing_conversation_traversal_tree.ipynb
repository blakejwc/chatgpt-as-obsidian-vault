{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Function to convert a timestamp to the required format\n",
    "def timestamp_to_str(timestamp):\n",
    "    if timestamp is None:\n",
    "        return 'None'\n",
    "    return datetime.utcfromtimestamp(timestamp).strftime('%y-%m-%d %H:%M')\n",
    "\n",
    "# Load the data\n",
    "with open('../data/conversations.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Conversation.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  conversation_id: 930bb6fd-00e1-4abc-8ab9-5f77c39bbb28\n",
      "DEBUG:root:  conversation_template_id: None\n",
      "DEBUG:root:  create_time: 1693281218.581128\n",
      "DEBUG:root:  current_node: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:  id: 930bb6fd-00e1-4abc-8ab9-5f77c39bbb28\n",
      "DEBUG:root:  mapping:\n",
      "DEBUG:root:    29bde5f1-d6d7-416e-8fb7-b2bf6c5db465:\n",
      "DEBUG:root:      children: []\n",
      "DEBUG:root:      id: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: assistant\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: text\n",
      "DEBUG:root:          parts: [\"Of course! Incorporating a reference to the JSON schema for ChatGPT chat exports will provide clarity and context for the developers of the Obsidian AI Research Assistant. Here's the updated feature request draft:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\nFor a detailed structure of the ChatGPT JSON data export, you can refer to this [JSON schema](https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json).\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nThis reference should help the developers understand the structure and specifics of the ChatGPT data export format, facilitating the development process for the desired feature.\"]\n",
      "DEBUG:root:        create_time: 1693281747.092052\n",
      "DEBUG:root:        end_turn: True\n",
      "DEBUG:root:        id: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:          finish_details:\n",
      "DEBUG:root:            stop_tokens: [100260]\n",
      "DEBUG:root:            type: stop\n",
      "DEBUG:root:          is_complete: True\n",
      "DEBUG:root:          message_type: None\n",
      "DEBUG:root:          model_slug: gpt-4-code-interpreter\n",
      "DEBUG:root:          parent_id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:          timestamp_: absolute\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 1.0\n",
      "DEBUG:root:      parent: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:    55308928-5076-4c81-bbfd-4bc504b237eb:\n",
      "DEBUG:root:      children: ['aaa2bd91-80e5-456c-8f73-6f0cf6adcd15']\n",
      "DEBUG:root:      id: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: system\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: text\n",
      "DEBUG:root:          parts: ['']\n",
      "DEBUG:root:        create_time: None\n",
      "DEBUG:root:        end_turn: True\n",
      "DEBUG:root:        id: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 0.0\n",
      "DEBUG:root:      parent: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:root:    6d02e2fb-8fc8-4f79-897c-ce1a736210b6:\n",
      "DEBUG:root:      children: ['aaa24f97-0b99-4e65-be87-ef5225236595']\n",
      "DEBUG:root:      id: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: assistant\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: text\n",
      "DEBUG:root:          parts: [\"Based on the information provided and the feature you want to request, here's a feature request draft for the Obsidian AI Research Assistant plugin:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nYou can use this draft as a starting point and customize it further based on your specific needs before submitting it as a feature request.\"]\n",
      "DEBUG:root:        create_time: 1693281237.862037\n",
      "DEBUG:root:        end_turn: True\n",
      "DEBUG:root:        id: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:          finish_details:\n",
      "DEBUG:root:            stop_tokens: [100260]\n",
      "DEBUG:root:            type: stop\n",
      "DEBUG:root:          is_complete: True\n",
      "DEBUG:root:          message_type: None\n",
      "DEBUG:root:          model_slug: gpt-4-code-interpreter\n",
      "DEBUG:root:          parent_id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:          timestamp_: absolute\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 1.0\n",
      "DEBUG:root:      parent: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:    aaa13816-46a6-4b26-ac95-dc204c5220ef:\n",
      "DEBUG:root:      children: ['55308928-5076-4c81-bbfd-4bc504b237eb']\n",
      "DEBUG:root:      id: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:root:      message: None\n",
      "DEBUG:root:      parent: None\n",
      "DEBUG:root:    aaa24f97-0b99-4e65-be87-ef5225236595:\n",
      "DEBUG:root:      children: ['29bde5f1-d6d7-416e-8fb7-b2bf6c5db465']\n",
      "DEBUG:root:      id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: user\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: text\n",
      "DEBUG:root:          parts: ['Could you please add a minor reference to https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json, which is as complete of a description of ? ']\n",
      "DEBUG:root:        create_time: 1693281722.670299\n",
      "DEBUG:root:        end_turn: None\n",
      "DEBUG:root:        id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:          message_type: None\n",
      "DEBUG:root:          timestamp_: absolute\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 1.0\n",
      "DEBUG:root:      parent: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:    aaa2bd91-80e5-456c-8f73-6f0cf6adcd15:\n",
      "DEBUG:root:      children: ['6d02e2fb-8fc8-4f79-897c-ce1a736210b6']\n",
      "DEBUG:root:      id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: user\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: text\n",
      "DEBUG:root:          parts: [\"I need help writing a feature request for a GitHub project I'm really excited about. The project is called Obsidian AI Research Assistant and is a plugin for Obsidian. I'm include the default text from the feature request below, delineated with triple backticks. I've included the project README.md below and delineated it with triple hyphens. \\n\\nThe feature I want to request is to take a ChatGPT data export JSON file and transform it into the AI Research Assistant's chat history format so that I can view all the chats I've exported. \\n\\nFeature request default text: ```\\n**Is your feature request related to a problem? Please describe.** A clear and concise description\\nof what the problem is. Ex. I'm always frustrated when [...]\\n\\n**Describe the solution you'd like** A clear and concise description of what you want to happen.\\n\\n**Describe alternatives you've considered** A clear and concise description of any alternative\\nsolutions or features you've considered.\\n\\n**Additional context** Add any other context or screenshots about the feature request here.\\n```\\n\\nREADME.md file:\\n---\\n# Obsidian AI Research Assistant\\n\\n![Forgetting a previous message](./docs/assets/forget-memory.png)\\n\\n> Build better Prompts and AI integrations with this advanced research tool for Prompt Engineering with the ChatGPT API.\\n\\n**Note**: This plugin is still in development and is not considered fully stable yet. **This is Beta\\nsoftware** and may contain bugs and unexpected behaviors. Please report\\n[Issues](https:/github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues) you find and feel\\nencouraged to [contribute](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/blob/main/docs/CONTRIBUTING.md) to the project.\\n\\n**Table of Contents**:\\n\\n- [Obsidian AI Research Assistant](#obsidian-ai-research-assistant)\\n  - [Summary](#summary)\\n  - [Features](#features)\\n  - [Roadmap](#roadmap)\\n    - [Probably Not on the Roadmap](#probably-not-on-the-roadmap)\\n  - [Installation](#installation)\\n    - [Other Installation Methods](#other-installation-methods)\\n    - [BRAT Installation](#brat-installation)\\n    - [Manual Installation](#manual-installation)\\n  - [Currently Supported Models](#currently-supported-models)\\n    - [Upcoming Model Support](#upcoming-model-support)\\n  - [Naming Conventions](#naming-conventions)\\n  - [Memories](#memories)\\n    - [Memory States](#memory-states)\\n\\n## Summary\\n\\nThis plugin integrates tools for Prompt Engineering and researching AI tools and language models\\nlike OpenAI's ChatGPT into Obsidian.\\n\\n![Basic interface and conversation summary](./docs/assets/basic-view.png)\\n\\nIt allows you to manually or automatically save your conversations with AI models, and then use\\nObsidian's powerful search and tagging features to organize and analyze them.\\n\\n![View the Preamble for the conversation](./docs/assets/preamble-summary.png)\\n\\nThe embedded Memory Manager allows you to edit which messages are included in the conversation's context, and you can even switch models mid-conversation to compare the output of various models to the same inputs.\\n\\n![Marking a previous message as a Core Memory](./docs/assets/core-memory.png)\\n\\nIt allows you to review, search, tag, and link your conversations in Obsidian, and view a summary of\\nthe conversation details its raw inputs and outputs.\\n\\n![View the conversational exchange](./docs/assets/conversation-view.png)\\n\\n![View the actual input sent to the API for requests](./docs/assets/raw-user-input.png)\\n\\n![View the raw JSON from the API for responses](./docs/assets/raw-response.png)\\n\\n## Features\\n\\n- Save (automatically or manually) conversations with AI models to Obsidian notes\\n  - Conversations become searchable, taggagle, and linkable in Obsidian\\n- Live edit which previous messages are used as Context for each Prompt\\n  - Mark a message as a **Core Memory** to make sure it's always in the Context\\n  - Mark a message as **Forgotten** to make sure it's never in the Context\\n  - Mark a message as **Remembered** to prioritize it's inclusion in the Context\\n  - By default the most recent messages are more likely to be remembered and older messages are forgotten as the conversation nears the memory and token limits\\n- Live edit the Preamble that is used for each Conversation\\n- Live edit the Prompt that is used for each Conversation\\n- View the raw JSON from the API for each response\\n- View the prompt and context that was sent to the API for each request\\n\\n## Roadmap\\n\\n- **Token-aware Memories**: Memory will be constructed within the desired amount of tokens\\n- **Configurable Response Token Buffer**: Ensure that the API has enough tokens to respond to your prompts\\n- **Models & APIs**: [More models](#upcoming-model-support)\\n- **Resume Conversations** Load conversations from Obsidian notes to pick up where you left off\\n- **Annotate Conversations**: Incrementally update notes on save (instead of overwriting the whole note on each save)\\n- **Title Generator**: Automatically generate a title based on the conversation (like ChatGPT)\\n- **Conversation Presets**: Save and Load preset configurations of Prefix, Model, Preamble, Context, etc. to make it easier to explore different research threads\\n- **LangChain Support**: Integrate with [LangChain](https://langchain.readthedocs.io/) via [langchan.js](https://github.com/hwchase17/langchainjs)\\n- **Features You Want**: [Request a Feature](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BRequest%5D%3A+)\\n\\n### Probably Not on the Roadmap\\n\\n- **Mobile Support**: Use AI Research Assistant on your mobile devices, too\\n  - Due to how Obsidian plugins handle storage of things like OpenAI API keys, there currently isn't a safe way to provide user's with an option to securely store their API key in a way that can be synced to mobile devices, too.\\n  - On desktop devices, Obsidian uses [Electron](https://www.electronjs.org/) and this plugin relies on Electron's [`safeStorage()` API](https://www.electronjs.org/docs/latest/api/safe-storage) to store secrets like OpenAI API Keys.\\n\\n## Installation\\n\\nThis plugin is now avaialble in the Obsidian Community Plugins Directory, and can be installed directly from the Obsidian UI.\\n\\n1. Open Obsidian's Settings\\n2. Click on **Community Plugins**\\n3. Click on **Browse** to open the Community Plugins Directory\\n4. Search for `AI Research Assistant`\\n5. Click on the plugin to open its page\\n6. Click **Install**\\n7. Return to the **Community Plugins** settings in Obsidian\\n8. Toggle the switch to enable `AI Research Assistant`\\n9. Configure `AI Research Assistant` in Obsidian's Settings\\n10. Enter your OpenAI API Key in the plugin's settings\\n11. Click the **Save API Key** button to save your API Key to Obsidian's secure storage\\n12. Click on the `AI Research Assistant` icon in the left sidebar to open the plugin and start chatting\\n\\n### Other Installation Methods\\n\\nYou can also [install it manually](#manual-installation) or [install it via the Beta Reviewer's Auto-update Tool (BRAT)](#brat-installation), though the official Community Plugin Directory is the recommended installation method.\\n\\n### BRAT Installation\\n\\n1. Install [BRAT](https://github.com/TfTHacker/obsidian42-brat)\\n2. Click the `Add Beta Plugin` button in BRAT's settings\\n3. Enter this repository's URL: `https://github.com/InterwebAlchemy/obsidian-ai-research-assistant`\\n4. Enable the plugin in Obsidian's Community Plugins settings\\n5. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n### Manual Installation\\n\\n1. Download the latest\\n   [release](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/releases)\\n2. Unzip the release into your Obsidian vault's `plugins` folder\\n3. Enable the plugin in Obsidian's Community Plugins settings\\n4. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n## Currently Supported Models\\n\\n- [OpenAI](https://platform.openai.com/docs/models)\\n  - [`gpt-4`](https://platform.openai.com/docs/models/gpt-4)\\n  - [`gpt-3.5-turbo`](https://platform.openai.com/docs/models/gpt-3-5)\\n\\n### Upcoming Model Support\\n\\n- [OpenAI Legacy Completions API](https://platform.openai.com/docs/deprecations/instructgpt-models)\\n  - `gpt-3.5-turbo-instruct`\\n  - **Note**: This plugin was originally built when only the completions API was available and was built to support the `text-davinci-003` model, but with recent changes in the OpenAI API, it is now recommended to use the `gpt-3.5-turbo` model instead and `text-daivinci-003` is being deprecated, so that functionality has been temporarily disabled in this plugin.\\n- Llama2\\n- Anthropic Claude\\n\\n## Naming Conventions\\n\\nThis plugin uses the following naming conventions to refer to different pieces of a conversation and\\nmake sure that it is always clear what is being referred to:\\n\\n**Note**: This plugin makes a distinction between a `Prompt` and a `Preamble` because it can be used\\nto generate prompts for models without a Preamble and in that case it is harder to distinguish\\nbetween what you might be editing when you click on an `Edit Prompt` button.\\n\\n- **Conversation**: This is an ongoing exchange of messages between the Human and the AI.\\n- **Preamble**: This is the initial instructions that a language model recieves. It is usually a\\n  short description of the topic of the conversation, and is used to provide context on how the\\n  model should behave, what the models knows, and how it should respond.\\n  - **Notable Examples**:\\n    - [ChatGPT](https://twitter.com/goodside/status/1598253337400717313)\\n    - [Perplexity AI](https://twitter.com/jmilldotdev/status/1600624362394091523)\\n    - [Bing Chat](https://twitter.com/kliu128/status/1623472922374574080)\\n- **Prompt**: Prompts are the questions that the model is asked to answer. They are usually a single\\n  sentence or a short paragraph.\\n  - **Notable Examples**:\\n    - [ShareGPT](https://sharegpt.com/)\\n    - [OpenAI Examples](https://platform.openai.com/examples/)\\n- **Context**: Context is the memory that the model uses to generate its response. It usually\\n  consists of the Preamble and some previous messages (or summaries them), and older messages are\\n  eventually replaced by newer ones as the conversation progresses and tokens becomes more limited.\\n- **Prefixes**: Prefixes are prepended to a Prompt and are generally used to include a Start Word\\n  that identifies the start of a Prompt and a Handle that identifies the speaker.\\n- **Suffixes**: Suffixes are appended to a Prompt and are generally used to include a Stop Word that\\n  identifies the end of a Prompt.\\n- **Handles**: Handles are short identifiers used to differentiate between speakers in the Context.\\n  By default this plugin uses `You:` to represent the Human and `AI:` to represent the AI model. You\\n  can change these prefixes in the AI Research Assistant settings.\\n- **Memories**: When [Memories](#memories) are enabled, the Context is generated from the Preamble\\n  and a defined number of previous messages. If the experimental Memory Manager is enabled, you can\\n  edit the Context in real time for each Prompt by defining memory staties for previous messages.\\n\\n## Memories\\n\\nMemories give your Conversation context and allow the language model to use what's already been said\\nto inform future responses.\\n\\nThere are four different [Memory States](#memory-states), and each message can have one of them:\\n\\nYou can manage the Memories of a Conversation in real time by clicking on the Memory State button\\nnext to each message bubble and choosing a new state for that memory.\\n\\n### Memory States\\n\\n- **Default**: By default, messages are **somtimes** remembered (in reverse chronological order),\\n  the most recent Memories are most likely to be remembered and older Memories are the first to be\\n  excluded frm the Conversation's Context. These Memories follow a basic First In First Out (FIFO)\\n  queue.\\n- **Core Memory**: Core Memories are **always** included in the Conversation's Context. They are\\n  usually the most important messages in the conversation and are used to provide consistent Context\\n  for the model. They do not count towards the maximum number of memories for a Conversation\\n  configured in the plugin's settings.\\n- **Remembered**: Remembered Memories **often** included in the Conversation's Context, until your\\n  Conversation reaches the maximum number of memories configured in the plugin's settings. They are\\n  usually more important messages, but not as important as Core Memories. They do count towards the\\n  maximum number of memories, and are accessed in reverse chronological order. If the maximum number\\n  of memories is reached, the oldest Remembered Memories will not be included in the Context.\\n- **Forgotten**: Forgotten Memories are **never** included in the Conversation's Context. You can\\n  forget a Memory at any time during the Conversation, and also restore Forgotten Memories to\\n  another Memory State.\\n---\"]\n",
      "DEBUG:root:        create_time: 1693281218.583268\n",
      "DEBUG:root:        end_turn: None\n",
      "DEBUG:root:        id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:          message_type: None\n",
      "DEBUG:root:          timestamp_: absolute\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 1.0\n",
      "DEBUG:root:      parent: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:  moderation_results: []\n",
      "DEBUG:root:  plugin_ids: None\n",
      "DEBUG:root:  title: Import ChatGPT JSON Format\n",
      "DEBUG:root:  update_time: 1693281747.0\n",
      "DEBUG:root:MessageRecord.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  children: ['aaa2bd91-80e5-456c-8f73-6f0cf6adcd15']\n",
      "DEBUG:root:  id: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:  message:\n",
      "DEBUG:root:    author:\n",
      "DEBUG:root:      metadata:\n",
      "DEBUG:root:      name: None\n",
      "DEBUG:root:      role: system\n",
      "DEBUG:root:    content:\n",
      "DEBUG:root:      content_type: text\n",
      "DEBUG:root:      parts: ['']\n",
      "DEBUG:root:    create_time: None\n",
      "DEBUG:root:    end_turn: True\n",
      "DEBUG:root:    id: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:    metadata:\n",
      "DEBUG:root:    recipient: all\n",
      "DEBUG:root:    status: finished_successfully\n",
      "DEBUG:root:    update_time: None\n",
      "DEBUG:root:    weight: 0.0\n",
      "DEBUG:root:  parent: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:root:Author.from_dict got: {'role': 'system', 'name': None, 'metadata': {}}, which is of type: dict\n",
      "DEBUG:root:ChatMessage.from_dict got empty metadata. Setting to empty dict.\n",
      "DEBUG:root:Content.from_dict got: {'content_type': 'text', 'parts': ['']}, which is of type: dict\n",
      "DEBUG:root:Content.from_dict got a dict with no metadata. Setting to empty dict.\n",
      "DEBUG:root:ChatMessage.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  author: Author(role=<RoleType.SYSTEM: 'system'>, name=None, metadata={})\n",
      "DEBUG:root:  content: \n",
      "DEBUG:root:  create_time: None\n",
      "DEBUG:root:  end_turn: True\n",
      "DEBUG:root:  id: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:  metadata:\n",
      "DEBUG:root:  recipient: all\n",
      "DEBUG:root:  status: finished_successfully\n",
      "DEBUG:root:  update_time: None\n",
      "DEBUG:root:  weight: 0.0\n",
      "DEBUG:root:MessageRecord.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  children: ['55308928-5076-4c81-bbfd-4bc504b237eb']\n",
      "DEBUG:root:  id: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:root:  message: None\n",
      "DEBUG:root:  parent: None\n",
      "DEBUG:root:MessageRecord.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  children: ['6d02e2fb-8fc8-4f79-897c-ce1a736210b6']\n",
      "DEBUG:root:  id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:  message:\n",
      "DEBUG:root:    author:\n",
      "DEBUG:root:      metadata:\n",
      "DEBUG:root:      name: None\n",
      "DEBUG:root:      role: user\n",
      "DEBUG:root:    content:\n",
      "DEBUG:root:      content_type: text\n",
      "DEBUG:root:      parts: [\"I need help writing a feature request for a GitHub project I'm really excited about. The project is called Obsidian AI Research Assistant and is a plugin for Obsidian. I'm include the default text from the feature request below, delineated with triple backticks. I've included the project README.md below and delineated it with triple hyphens. \\n\\nThe feature I want to request is to take a ChatGPT data export JSON file and transform it into the AI Research Assistant's chat history format so that I can view all the chats I've exported. \\n\\nFeature request default text: ```\\n**Is your feature request related to a problem? Please describe.** A clear and concise description\\nof what the problem is. Ex. I'm always frustrated when [...]\\n\\n**Describe the solution you'd like** A clear and concise description of what you want to happen.\\n\\n**Describe alternatives you've considered** A clear and concise description of any alternative\\nsolutions or features you've considered.\\n\\n**Additional context** Add any other context or screenshots about the feature request here.\\n```\\n\\nREADME.md file:\\n---\\n# Obsidian AI Research Assistant\\n\\n![Forgetting a previous message](./docs/assets/forget-memory.png)\\n\\n> Build better Prompts and AI integrations with this advanced research tool for Prompt Engineering with the ChatGPT API.\\n\\n**Note**: This plugin is still in development and is not considered fully stable yet. **This is Beta\\nsoftware** and may contain bugs and unexpected behaviors. Please report\\n[Issues](https:/github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues) you find and feel\\nencouraged to [contribute](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/blob/main/docs/CONTRIBUTING.md) to the project.\\n\\n**Table of Contents**:\\n\\n- [Obsidian AI Research Assistant](#obsidian-ai-research-assistant)\\n  - [Summary](#summary)\\n  - [Features](#features)\\n  - [Roadmap](#roadmap)\\n    - [Probably Not on the Roadmap](#probably-not-on-the-roadmap)\\n  - [Installation](#installation)\\n    - [Other Installation Methods](#other-installation-methods)\\n    - [BRAT Installation](#brat-installation)\\n    - [Manual Installation](#manual-installation)\\n  - [Currently Supported Models](#currently-supported-models)\\n    - [Upcoming Model Support](#upcoming-model-support)\\n  - [Naming Conventions](#naming-conventions)\\n  - [Memories](#memories)\\n    - [Memory States](#memory-states)\\n\\n## Summary\\n\\nThis plugin integrates tools for Prompt Engineering and researching AI tools and language models\\nlike OpenAI's ChatGPT into Obsidian.\\n\\n![Basic interface and conversation summary](./docs/assets/basic-view.png)\\n\\nIt allows you to manually or automatically save your conversations with AI models, and then use\\nObsidian's powerful search and tagging features to organize and analyze them.\\n\\n![View the Preamble for the conversation](./docs/assets/preamble-summary.png)\\n\\nThe embedded Memory Manager allows you to edit which messages are included in the conversation's context, and you can even switch models mid-conversation to compare the output of various models to the same inputs.\\n\\n![Marking a previous message as a Core Memory](./docs/assets/core-memory.png)\\n\\nIt allows you to review, search, tag, and link your conversations in Obsidian, and view a summary of\\nthe conversation details its raw inputs and outputs.\\n\\n![View the conversational exchange](./docs/assets/conversation-view.png)\\n\\n![View the actual input sent to the API for requests](./docs/assets/raw-user-input.png)\\n\\n![View the raw JSON from the API for responses](./docs/assets/raw-response.png)\\n\\n## Features\\n\\n- Save (automatically or manually) conversations with AI models to Obsidian notes\\n  - Conversations become searchable, taggagle, and linkable in Obsidian\\n- Live edit which previous messages are used as Context for each Prompt\\n  - Mark a message as a **Core Memory** to make sure it's always in the Context\\n  - Mark a message as **Forgotten** to make sure it's never in the Context\\n  - Mark a message as **Remembered** to prioritize it's inclusion in the Context\\n  - By default the most recent messages are more likely to be remembered and older messages are forgotten as the conversation nears the memory and token limits\\n- Live edit the Preamble that is used for each Conversation\\n- Live edit the Prompt that is used for each Conversation\\n- View the raw JSON from the API for each response\\n- View the prompt and context that was sent to the API for each request\\n\\n## Roadmap\\n\\n- **Token-aware Memories**: Memory will be constructed within the desired amount of tokens\\n- **Configurable Response Token Buffer**: Ensure that the API has enough tokens to respond to your prompts\\n- **Models & APIs**: [More models](#upcoming-model-support)\\n- **Resume Conversations** Load conversations from Obsidian notes to pick up where you left off\\n- **Annotate Conversations**: Incrementally update notes on save (instead of overwriting the whole note on each save)\\n- **Title Generator**: Automatically generate a title based on the conversation (like ChatGPT)\\n- **Conversation Presets**: Save and Load preset configurations of Prefix, Model, Preamble, Context, etc. to make it easier to explore different research threads\\n- **LangChain Support**: Integrate with [LangChain](https://langchain.readthedocs.io/) via [langchan.js](https://github.com/hwchase17/langchainjs)\\n- **Features You Want**: [Request a Feature](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BRequest%5D%3A+)\\n\\n### Probably Not on the Roadmap\\n\\n- **Mobile Support**: Use AI Research Assistant on your mobile devices, too\\n  - Due to how Obsidian plugins handle storage of things like OpenAI API keys, there currently isn't a safe way to provide user's with an option to securely store their API key in a way that can be synced to mobile devices, too.\\n  - On desktop devices, Obsidian uses [Electron](https://www.electronjs.org/) and this plugin relies on Electron's [`safeStorage()` API](https://www.electronjs.org/docs/latest/api/safe-storage) to store secrets like OpenAI API Keys.\\n\\n## Installation\\n\\nThis plugin is now avaialble in the Obsidian Community Plugins Directory, and can be installed directly from the Obsidian UI.\\n\\n1. Open Obsidian's Settings\\n2. Click on **Community Plugins**\\n3. Click on **Browse** to open the Community Plugins Directory\\n4. Search for `AI Research Assistant`\\n5. Click on the plugin to open its page\\n6. Click **Install**\\n7. Return to the **Community Plugins** settings in Obsidian\\n8. Toggle the switch to enable `AI Research Assistant`\\n9. Configure `AI Research Assistant` in Obsidian's Settings\\n10. Enter your OpenAI API Key in the plugin's settings\\n11. Click the **Save API Key** button to save your API Key to Obsidian's secure storage\\n12. Click on the `AI Research Assistant` icon in the left sidebar to open the plugin and start chatting\\n\\n### Other Installation Methods\\n\\nYou can also [install it manually](#manual-installation) or [install it via the Beta Reviewer's Auto-update Tool (BRAT)](#brat-installation), though the official Community Plugin Directory is the recommended installation method.\\n\\n### BRAT Installation\\n\\n1. Install [BRAT](https://github.com/TfTHacker/obsidian42-brat)\\n2. Click the `Add Beta Plugin` button in BRAT's settings\\n3. Enter this repository's URL: `https://github.com/InterwebAlchemy/obsidian-ai-research-assistant`\\n4. Enable the plugin in Obsidian's Community Plugins settings\\n5. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n### Manual Installation\\n\\n1. Download the latest\\n   [release](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/releases)\\n2. Unzip the release into your Obsidian vault's `plugins` folder\\n3. Enable the plugin in Obsidian's Community Plugins settings\\n4. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n## Currently Supported Models\\n\\n- [OpenAI](https://platform.openai.com/docs/models)\\n  - [`gpt-4`](https://platform.openai.com/docs/models/gpt-4)\\n  - [`gpt-3.5-turbo`](https://platform.openai.com/docs/models/gpt-3-5)\\n\\n### Upcoming Model Support\\n\\n- [OpenAI Legacy Completions API](https://platform.openai.com/docs/deprecations/instructgpt-models)\\n  - `gpt-3.5-turbo-instruct`\\n  - **Note**: This plugin was originally built when only the completions API was available and was built to support the `text-davinci-003` model, but with recent changes in the OpenAI API, it is now recommended to use the `gpt-3.5-turbo` model instead and `text-daivinci-003` is being deprecated, so that functionality has been temporarily disabled in this plugin.\\n- Llama2\\n- Anthropic Claude\\n\\n## Naming Conventions\\n\\nThis plugin uses the following naming conventions to refer to different pieces of a conversation and\\nmake sure that it is always clear what is being referred to:\\n\\n**Note**: This plugin makes a distinction between a `Prompt` and a `Preamble` because it can be used\\nto generate prompts for models without a Preamble and in that case it is harder to distinguish\\nbetween what you might be editing when you click on an `Edit Prompt` button.\\n\\n- **Conversation**: This is an ongoing exchange of messages between the Human and the AI.\\n- **Preamble**: This is the initial instructions that a language model recieves. It is usually a\\n  short description of the topic of the conversation, and is used to provide context on how the\\n  model should behave, what the models knows, and how it should respond.\\n  - **Notable Examples**:\\n    - [ChatGPT](https://twitter.com/goodside/status/1598253337400717313)\\n    - [Perplexity AI](https://twitter.com/jmilldotdev/status/1600624362394091523)\\n    - [Bing Chat](https://twitter.com/kliu128/status/1623472922374574080)\\n- **Prompt**: Prompts are the questions that the model is asked to answer. They are usually a single\\n  sentence or a short paragraph.\\n  - **Notable Examples**:\\n    - [ShareGPT](https://sharegpt.com/)\\n    - [OpenAI Examples](https://platform.openai.com/examples/)\\n- **Context**: Context is the memory that the model uses to generate its response. It usually\\n  consists of the Preamble and some previous messages (or summaries them), and older messages are\\n  eventually replaced by newer ones as the conversation progresses and tokens becomes more limited.\\n- **Prefixes**: Prefixes are prepended to a Prompt and are generally used to include a Start Word\\n  that identifies the start of a Prompt and a Handle that identifies the speaker.\\n- **Suffixes**: Suffixes are appended to a Prompt and are generally used to include a Stop Word that\\n  identifies the end of a Prompt.\\n- **Handles**: Handles are short identifiers used to differentiate between speakers in the Context.\\n  By default this plugin uses `You:` to represent the Human and `AI:` to represent the AI model. You\\n  can change these prefixes in the AI Research Assistant settings.\\n- **Memories**: When [Memories](#memories) are enabled, the Context is generated from the Preamble\\n  and a defined number of previous messages. If the experimental Memory Manager is enabled, you can\\n  edit the Context in real time for each Prompt by defining memory staties for previous messages.\\n\\n## Memories\\n\\nMemories give your Conversation context and allow the language model to use what's already been said\\nto inform future responses.\\n\\nThere are four different [Memory States](#memory-states), and each message can have one of them:\\n\\nYou can manage the Memories of a Conversation in real time by clicking on the Memory State button\\nnext to each message bubble and choosing a new state for that memory.\\n\\n### Memory States\\n\\n- **Default**: By default, messages are **somtimes** remembered (in reverse chronological order),\\n  the most recent Memories are most likely to be remembered and older Memories are the first to be\\n  excluded frm the Conversation's Context. These Memories follow a basic First In First Out (FIFO)\\n  queue.\\n- **Core Memory**: Core Memories are **always** included in the Conversation's Context. They are\\n  usually the most important messages in the conversation and are used to provide consistent Context\\n  for the model. They do not count towards the maximum number of memories for a Conversation\\n  configured in the plugin's settings.\\n- **Remembered**: Remembered Memories **often** included in the Conversation's Context, until your\\n  Conversation reaches the maximum number of memories configured in the plugin's settings. They are\\n  usually more important messages, but not as important as Core Memories. They do count towards the\\n  maximum number of memories, and are accessed in reverse chronological order. If the maximum number\\n  of memories is reached, the oldest Remembered Memories will not be included in the Context.\\n- **Forgotten**: Forgotten Memories are **never** included in the Conversation's Context. You can\\n  forget a Memory at any time during the Conversation, and also restore Forgotten Memories to\\n  another Memory State.\\n---\"]\n",
      "DEBUG:root:    create_time: 1693281218.583268\n",
      "DEBUG:root:    end_turn: None\n",
      "DEBUG:root:    id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:    metadata:\n",
      "DEBUG:root:      message_type: None\n",
      "DEBUG:root:      timestamp_: absolute\n",
      "DEBUG:root:    recipient: all\n",
      "DEBUG:root:    status: finished_successfully\n",
      "DEBUG:root:    update_time: None\n",
      "DEBUG:root:    weight: 1.0\n",
      "DEBUG:root:  parent: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:Author.from_dict got: {'role': 'user', 'name': None, 'metadata': {}}, which is of type: dict\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got: {'timestamp_': 'absolute', 'message_type': None}, which is of type: dict\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got no `finish_details`. Setting to None.\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got no `model_slug`. Setting to None.\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got no `is_complete`. Setting to None.\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got no `parent_id`. Setting to None.\n",
      "DEBUG:root:Content.from_dict got: {'content_type': 'text', 'parts': [\"I need help writing a feature request for a GitHub project I'm really excited about. The project is called Obsidian AI Research Assistant and is a plugin for Obsidian. I'm include the default text from the feature request below, delineated with triple backticks. I've included the project README.md below and delineated it with triple hyphens. \\n\\nThe feature I want to request is to take a ChatGPT data export JSON file and transform it into the AI Research Assistant's chat history format so that I can view all the chats I've exported. \\n\\nFeature request default text: ```\\n**Is your feature request related to a problem? Please describe.** A clear and concise description\\nof what the problem is. Ex. I'm always frustrated when [...]\\n\\n**Describe the solution you'd like** A clear and concise description of what you want to happen.\\n\\n**Describe alternatives you've considered** A clear and concise description of any alternative\\nsolutions or features you've considered.\\n\\n**Additional context** Add any other context or screenshots about the feature request here.\\n```\\n\\nREADME.md file:\\n---\\n# Obsidian AI Research Assistant\\n\\n![Forgetting a previous message](./docs/assets/forget-memory.png)\\n\\n> Build better Prompts and AI integrations with this advanced research tool for Prompt Engineering with the ChatGPT API.\\n\\n**Note**: This plugin is still in development and is not considered fully stable yet. **This is Beta\\nsoftware** and may contain bugs and unexpected behaviors. Please report\\n[Issues](https:/github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues) you find and feel\\nencouraged to [contribute](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/blob/main/docs/CONTRIBUTING.md) to the project.\\n\\n**Table of Contents**:\\n\\n- [Obsidian AI Research Assistant](#obsidian-ai-research-assistant)\\n  - [Summary](#summary)\\n  - [Features](#features)\\n  - [Roadmap](#roadmap)\\n    - [Probably Not on the Roadmap](#probably-not-on-the-roadmap)\\n  - [Installation](#installation)\\n    - [Other Installation Methods](#other-installation-methods)\\n    - [BRAT Installation](#brat-installation)\\n    - [Manual Installation](#manual-installation)\\n  - [Currently Supported Models](#currently-supported-models)\\n    - [Upcoming Model Support](#upcoming-model-support)\\n  - [Naming Conventions](#naming-conventions)\\n  - [Memories](#memories)\\n    - [Memory States](#memory-states)\\n\\n## Summary\\n\\nThis plugin integrates tools for Prompt Engineering and researching AI tools and language models\\nlike OpenAI's ChatGPT into Obsidian.\\n\\n![Basic interface and conversation summary](./docs/assets/basic-view.png)\\n\\nIt allows you to manually or automatically save your conversations with AI models, and then use\\nObsidian's powerful search and tagging features to organize and analyze them.\\n\\n![View the Preamble for the conversation](./docs/assets/preamble-summary.png)\\n\\nThe embedded Memory Manager allows you to edit which messages are included in the conversation's context, and you can even switch models mid-conversation to compare the output of various models to the same inputs.\\n\\n![Marking a previous message as a Core Memory](./docs/assets/core-memory.png)\\n\\nIt allows you to review, search, tag, and link your conversations in Obsidian, and view a summary of\\nthe conversation details its raw inputs and outputs.\\n\\n![View the conversational exchange](./docs/assets/conversation-view.png)\\n\\n![View the actual input sent to the API for requests](./docs/assets/raw-user-input.png)\\n\\n![View the raw JSON from the API for responses](./docs/assets/raw-response.png)\\n\\n## Features\\n\\n- Save (automatically or manually) conversations with AI models to Obsidian notes\\n  - Conversations become searchable, taggagle, and linkable in Obsidian\\n- Live edit which previous messages are used as Context for each Prompt\\n  - Mark a message as a **Core Memory** to make sure it's always in the Context\\n  - Mark a message as **Forgotten** to make sure it's never in the Context\\n  - Mark a message as **Remembered** to prioritize it's inclusion in the Context\\n  - By default the most recent messages are more likely to be remembered and older messages are forgotten as the conversation nears the memory and token limits\\n- Live edit the Preamble that is used for each Conversation\\n- Live edit the Prompt that is used for each Conversation\\n- View the raw JSON from the API for each response\\n- View the prompt and context that was sent to the API for each request\\n\\n## Roadmap\\n\\n- **Token-aware Memories**: Memory will be constructed within the desired amount of tokens\\n- **Configurable Response Token Buffer**: Ensure that the API has enough tokens to respond to your prompts\\n- **Models & APIs**: [More models](#upcoming-model-support)\\n- **Resume Conversations** Load conversations from Obsidian notes to pick up where you left off\\n- **Annotate Conversations**: Incrementally update notes on save (instead of overwriting the whole note on each save)\\n- **Title Generator**: Automatically generate a title based on the conversation (like ChatGPT)\\n- **Conversation Presets**: Save and Load preset configurations of Prefix, Model, Preamble, Context, etc. to make it easier to explore different research threads\\n- **LangChain Support**: Integrate with [LangChain](https://langchain.readthedocs.io/) via [langchan.js](https://github.com/hwchase17/langchainjs)\\n- **Features You Want**: [Request a Feature](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BRequest%5D%3A+)\\n\\n### Probably Not on the Roadmap\\n\\n- **Mobile Support**: Use AI Research Assistant on your mobile devices, too\\n  - Due to how Obsidian plugins handle storage of things like OpenAI API keys, there currently isn't a safe way to provide user's with an option to securely store their API key in a way that can be synced to mobile devices, too.\\n  - On desktop devices, Obsidian uses [Electron](https://www.electronjs.org/) and this plugin relies on Electron's [`safeStorage()` API](https://www.electronjs.org/docs/latest/api/safe-storage) to store secrets like OpenAI API Keys.\\n\\n## Installation\\n\\nThis plugin is now avaialble in the Obsidian Community Plugins Directory, and can be installed directly from the Obsidian UI.\\n\\n1. Open Obsidian's Settings\\n2. Click on **Community Plugins**\\n3. Click on **Browse** to open the Community Plugins Directory\\n4. Search for `AI Research Assistant`\\n5. Click on the plugin to open its page\\n6. Click **Install**\\n7. Return to the **Community Plugins** settings in Obsidian\\n8. Toggle the switch to enable `AI Research Assistant`\\n9. Configure `AI Research Assistant` in Obsidian's Settings\\n10. Enter your OpenAI API Key in the plugin's settings\\n11. Click the **Save API Key** button to save your API Key to Obsidian's secure storage\\n12. Click on the `AI Research Assistant` icon in the left sidebar to open the plugin and start chatting\\n\\n### Other Installation Methods\\n\\nYou can also [install it manually](#manual-installation) or [install it via the Beta Reviewer's Auto-update Tool (BRAT)](#brat-installation), though the official Community Plugin Directory is the recommended installation method.\\n\\n### BRAT Installation\\n\\n1. Install [BRAT](https://github.com/TfTHacker/obsidian42-brat)\\n2. Click the `Add Beta Plugin` button in BRAT's settings\\n3. Enter this repository's URL: `https://github.com/InterwebAlchemy/obsidian-ai-research-assistant`\\n4. Enable the plugin in Obsidian's Community Plugins settings\\n5. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n### Manual Installation\\n\\n1. Download the latest\\n   [release](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/releases)\\n2. Unzip the release into your Obsidian vault's `plugins` folder\\n3. Enable the plugin in Obsidian's Community Plugins settings\\n4. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n## Currently Supported Models\\n\\n- [OpenAI](https://platform.openai.com/docs/models)\\n  - [`gpt-4`](https://platform.openai.com/docs/models/gpt-4)\\n  - [`gpt-3.5-turbo`](https://platform.openai.com/docs/models/gpt-3-5)\\n\\n### Upcoming Model Support\\n\\n- [OpenAI Legacy Completions API](https://platform.openai.com/docs/deprecations/instructgpt-models)\\n  - `gpt-3.5-turbo-instruct`\\n  - **Note**: This plugin was originally built when only the completions API was available and was built to support the `text-davinci-003` model, but with recent changes in the OpenAI API, it is now recommended to use the `gpt-3.5-turbo` model instead and `text-daivinci-003` is being deprecated, so that functionality has been temporarily disabled in this plugin.\\n- Llama2\\n- Anthropic Claude\\n\\n## Naming Conventions\\n\\nThis plugin uses the following naming conventions to refer to different pieces of a conversation and\\nmake sure that it is always clear what is being referred to:\\n\\n**Note**: This plugin makes a distinction between a `Prompt` and a `Preamble` because it can be used\\nto generate prompts for models without a Preamble and in that case it is harder to distinguish\\nbetween what you might be editing when you click on an `Edit Prompt` button.\\n\\n- **Conversation**: This is an ongoing exchange of messages between the Human and the AI.\\n- **Preamble**: This is the initial instructions that a language model recieves. It is usually a\\n  short description of the topic of the conversation, and is used to provide context on how the\\n  model should behave, what the models knows, and how it should respond.\\n  - **Notable Examples**:\\n    - [ChatGPT](https://twitter.com/goodside/status/1598253337400717313)\\n    - [Perplexity AI](https://twitter.com/jmilldotdev/status/1600624362394091523)\\n    - [Bing Chat](https://twitter.com/kliu128/status/1623472922374574080)\\n- **Prompt**: Prompts are the questions that the model is asked to answer. They are usually a single\\n  sentence or a short paragraph.\\n  - **Notable Examples**:\\n    - [ShareGPT](https://sharegpt.com/)\\n    - [OpenAI Examples](https://platform.openai.com/examples/)\\n- **Context**: Context is the memory that the model uses to generate its response. It usually\\n  consists of the Preamble and some previous messages (or summaries them), and older messages are\\n  eventually replaced by newer ones as the conversation progresses and tokens becomes more limited.\\n- **Prefixes**: Prefixes are prepended to a Prompt and are generally used to include a Start Word\\n  that identifies the start of a Prompt and a Handle that identifies the speaker.\\n- **Suffixes**: Suffixes are appended to a Prompt and are generally used to include a Stop Word that\\n  identifies the end of a Prompt.\\n- **Handles**: Handles are short identifiers used to differentiate between speakers in the Context.\\n  By default this plugin uses `You:` to represent the Human and `AI:` to represent the AI model. You\\n  can change these prefixes in the AI Research Assistant settings.\\n- **Memories**: When [Memories](#memories) are enabled, the Context is generated from the Preamble\\n  and a defined number of previous messages. If the experimental Memory Manager is enabled, you can\\n  edit the Context in real time for each Prompt by defining memory staties for previous messages.\\n\\n## Memories\\n\\nMemories give your Conversation context and allow the language model to use what's already been said\\nto inform future responses.\\n\\nThere are four different [Memory States](#memory-states), and each message can have one of them:\\n\\nYou can manage the Memories of a Conversation in real time by clicking on the Memory State button\\nnext to each message bubble and choosing a new state for that memory.\\n\\n### Memory States\\n\\n- **Default**: By default, messages are **somtimes** remembered (in reverse chronological order),\\n  the most recent Memories are most likely to be remembered and older Memories are the first to be\\n  excluded frm the Conversation's Context. These Memories follow a basic First In First Out (FIFO)\\n  queue.\\n- **Core Memory**: Core Memories are **always** included in the Conversation's Context. They are\\n  usually the most important messages in the conversation and are used to provide consistent Context\\n  for the model. They do not count towards the maximum number of memories for a Conversation\\n  configured in the plugin's settings.\\n- **Remembered**: Remembered Memories **often** included in the Conversation's Context, until your\\n  Conversation reaches the maximum number of memories configured in the plugin's settings. They are\\n  usually more important messages, but not as important as Core Memories. They do count towards the\\n  maximum number of memories, and are accessed in reverse chronological order. If the maximum number\\n  of memories is reached, the oldest Remembered Memories will not be included in the Context.\\n- **Forgotten**: Forgotten Memories are **never** included in the Conversation's Context. You can\\n  forget a Memory at any time during the Conversation, and also restore Forgotten Memories to\\n  another Memory State.\\n---\"]}, which is of type: dict\n",
      "DEBUG:root:Content.from_dict got a dict with no metadata. Setting to empty dict.\n",
      "DEBUG:root:ChatMessage.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  author: Author(role=<RoleType.USER: 'user'>, name=None, metadata={})\n",
      "DEBUG:root:  content: I need help writing a feature request for a GitHub project I'm really excited about. The project is called Obsidian AI Research Assistant and is a plugin for Obsidian. I'm include the default text from the feature request below, delineated with triple backticks. I've included the project README.md below and delineated it with triple hyphens. \n",
      "\n",
      "The feature I want to request is to take a ChatGPT data export JSON file and transform it into the AI Research Assistant's chat history format so that I can view all the chats I've exported. \n",
      "\n",
      "Feature request default text: ```\n",
      "**Is your feature request related to a problem? Please describe.** A clear and concise description\n",
      "of what the problem is. Ex. I'm always frustrated when [...]\n",
      "\n",
      "**Describe the solution you'd like** A clear and concise description of what you want to happen.\n",
      "\n",
      "**Describe alternatives you've considered** A clear and concise description of any alternative\n",
      "solutions or features you've considered.\n",
      "\n",
      "**Additional context** Add any other context or screenshots about the feature request here.\n",
      "```\n",
      "\n",
      "README.md file:\n",
      "---\n",
      "# Obsidian AI Research Assistant\n",
      "\n",
      "![Forgetting a previous message](./docs/assets/forget-memory.png)\n",
      "\n",
      "> Build better Prompts and AI integrations with this advanced research tool for Prompt Engineering with the ChatGPT API.\n",
      "\n",
      "**Note**: This plugin is still in development and is not considered fully stable yet. **This is Beta\n",
      "software** and may contain bugs and unexpected behaviors. Please report\n",
      "[Issues](https:/github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues) you find and feel\n",
      "encouraged to [contribute](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/blob/main/docs/CONTRIBUTING.md) to the project.\n",
      "\n",
      "**Table of Contents**:\n",
      "\n",
      "- [Obsidian AI Research Assistant](#obsidian-ai-research-assistant)\n",
      "  - [Summary](#summary)\n",
      "  - [Features](#features)\n",
      "  - [Roadmap](#roadmap)\n",
      "    - [Probably Not on the Roadmap](#probably-not-on-the-roadmap)\n",
      "  - [Installation](#installation)\n",
      "    - [Other Installation Methods](#other-installation-methods)\n",
      "    - [BRAT Installation](#brat-installation)\n",
      "    - [Manual Installation](#manual-installation)\n",
      "  - [Currently Supported Models](#currently-supported-models)\n",
      "    - [Upcoming Model Support](#upcoming-model-support)\n",
      "  - [Naming Conventions](#naming-conventions)\n",
      "  - [Memories](#memories)\n",
      "    - [Memory States](#memory-states)\n",
      "\n",
      "## Summary\n",
      "\n",
      "This plugin integrates tools for Prompt Engineering and researching AI tools and language models\n",
      "like OpenAI's ChatGPT into Obsidian.\n",
      "\n",
      "![Basic interface and conversation summary](./docs/assets/basic-view.png)\n",
      "\n",
      "It allows you to manually or automatically save your conversations with AI models, and then use\n",
      "Obsidian's powerful search and tagging features to organize and analyze them.\n",
      "\n",
      "![View the Preamble for the conversation](./docs/assets/preamble-summary.png)\n",
      "\n",
      "The embedded Memory Manager allows you to edit which messages are included in the conversation's context, and you can even switch models mid-conversation to compare the output of various models to the same inputs.\n",
      "\n",
      "![Marking a previous message as a Core Memory](./docs/assets/core-memory.png)\n",
      "\n",
      "It allows you to review, search, tag, and link your conversations in Obsidian, and view a summary of\n",
      "the conversation details its raw inputs and outputs.\n",
      "\n",
      "![View the conversational exchange](./docs/assets/conversation-view.png)\n",
      "\n",
      "![View the actual input sent to the API for requests](./docs/assets/raw-user-input.png)\n",
      "\n",
      "![View the raw JSON from the API for responses](./docs/assets/raw-response.png)\n",
      "\n",
      "## Features\n",
      "\n",
      "- Save (automatically or manually) conversations with AI models to Obsidian notes\n",
      "  - Conversations become searchable, taggagle, and linkable in Obsidian\n",
      "- Live edit which previous messages are used as Context for each Prompt\n",
      "  - Mark a message as a **Core Memory** to make sure it's always in the Context\n",
      "  - Mark a message as **Forgotten** to make sure it's never in the Context\n",
      "  - Mark a message as **Remembered** to prioritize it's inclusion in the Context\n",
      "  - By default the most recent messages are more likely to be remembered and older messages are forgotten as the conversation nears the memory and token limits\n",
      "- Live edit the Preamble that is used for each Conversation\n",
      "- Live edit the Prompt that is used for each Conversation\n",
      "- View the raw JSON from the API for each response\n",
      "- View the prompt and context that was sent to the API for each request\n",
      "\n",
      "## Roadmap\n",
      "\n",
      "- **Token-aware Memories**: Memory will be constructed within the desired amount of tokens\n",
      "- **Configurable Response Token Buffer**: Ensure that the API has enough tokens to respond to your prompts\n",
      "- **Models & APIs**: [More models](#upcoming-model-support)\n",
      "- **Resume Conversations** Load conversations from Obsidian notes to pick up where you left off\n",
      "- **Annotate Conversations**: Incrementally update notes on save (instead of overwriting the whole note on each save)\n",
      "- **Title Generator**: Automatically generate a title based on the conversation (like ChatGPT)\n",
      "- **Conversation Presets**: Save and Load preset configurations of Prefix, Model, Preamble, Context, etc. to make it easier to explore different research threads\n",
      "- **LangChain Support**: Integrate with [LangChain](https://langchain.readthedocs.io/) via [langchan.js](https://github.com/hwchase17/langchainjs)\n",
      "- **Features You Want**: [Request a Feature](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BRequest%5D%3A+)\n",
      "\n",
      "### Probably Not on the Roadmap\n",
      "\n",
      "- **Mobile Support**: Use AI Research Assistant on your mobile devices, too\n",
      "  - Due to how Obsidian plugins handle storage of things like OpenAI API keys, there currently isn't a safe way to provide user's with an option to securely store their API key in a way that can be synced to mobile devices, too.\n",
      "  - On desktop devices, Obsidian uses [Electron](https://www.electronjs.org/) and this plugin relies on Electron's [`safeStorage()` API](https://www.electronjs.org/docs/latest/api/safe-storage) to store secrets like OpenAI API Keys.\n",
      "\n",
      "## Installation\n",
      "\n",
      "This plugin is now avaialble in the Obsidian Community Plugins Directory, and can be installed directly from the Obsidian UI.\n",
      "\n",
      "1. Open Obsidian's Settings\n",
      "2. Click on **Community Plugins**\n",
      "3. Click on **Browse** to open the Community Plugins Directory\n",
      "4. Search for `AI Research Assistant`\n",
      "5. Click on the plugin to open its page\n",
      "6. Click **Install**\n",
      "7. Return to the **Community Plugins** settings in Obsidian\n",
      "8. Toggle the switch to enable `AI Research Assistant`\n",
      "9. Configure `AI Research Assistant` in Obsidian's Settings\n",
      "10. Enter your OpenAI API Key in the plugin's settings\n",
      "11. Click the **Save API Key** button to save your API Key to Obsidian's secure storage\n",
      "12. Click on the `AI Research Assistant` icon in the left sidebar to open the plugin and start chatting\n",
      "\n",
      "### Other Installation Methods\n",
      "\n",
      "You can also [install it manually](#manual-installation) or [install it via the Beta Reviewer's Auto-update Tool (BRAT)](#brat-installation), though the official Community Plugin Directory is the recommended installation method.\n",
      "\n",
      "### BRAT Installation\n",
      "\n",
      "1. Install [BRAT](https://github.com/TfTHacker/obsidian42-brat)\n",
      "2. Click the `Add Beta Plugin` button in BRAT's settings\n",
      "3. Enter this repository's URL: `https://github.com/InterwebAlchemy/obsidian-ai-research-assistant`\n",
      "4. Enable the plugin in Obsidian's Community Plugins settings\n",
      "5. Configure `AI Research Assistant` in Obsidian's Settings\n",
      "\n",
      "### Manual Installation\n",
      "\n",
      "1. Download the latest\n",
      "   [release](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/releases)\n",
      "2. Unzip the release into your Obsidian vault's `plugins` folder\n",
      "3. Enable the plugin in Obsidian's Community Plugins settings\n",
      "4. Configure `AI Research Assistant` in Obsidian's Settings\n",
      "\n",
      "## Currently Supported Models\n",
      "\n",
      "- [OpenAI](https://platform.openai.com/docs/models)\n",
      "  - [`gpt-4`](https://platform.openai.com/docs/models/gpt-4)\n",
      "  - [`gpt-3.5-turbo`](https://platform.openai.com/docs/models/gpt-3-5)\n",
      "\n",
      "### Upcoming Model Support\n",
      "\n",
      "- [OpenAI Legacy Completions API](https://platform.openai.com/docs/deprecations/instructgpt-models)\n",
      "  - `gpt-3.5-turbo-instruct`\n",
      "  - **Note**: This plugin was originally built when only the completions API was available and was built to support the `text-davinci-003` model, but with recent changes in the OpenAI API, it is now recommended to use the `gpt-3.5-turbo` model instead and `text-daivinci-003` is being deprecated, so that functionality has been temporarily disabled in this plugin.\n",
      "- Llama2\n",
      "- Anthropic Claude\n",
      "\n",
      "## Naming Conventions\n",
      "\n",
      "This plugin uses the following naming conventions to refer to different pieces of a conversation and\n",
      "make sure that it is always clear what is being referred to:\n",
      "\n",
      "**Note**: This plugin makes a distinction between a `Prompt` and a `Preamble` because it can be used\n",
      "to generate prompts for models without a Preamble and in that case it is harder to distinguish\n",
      "between what you might be editing when you click on an `Edit Prompt` button.\n",
      "\n",
      "- **Conversation**: This is an ongoing exchange of messages between the Human and the AI.\n",
      "- **Preamble**: This is the initial instructions that a language model recieves. It is usually a\n",
      "  short description of the topic of the conversation, and is used to provide context on how the\n",
      "  model should behave, what the models knows, and how it should respond.\n",
      "  - **Notable Examples**:\n",
      "    - [ChatGPT](https://twitter.com/goodside/status/1598253337400717313)\n",
      "    - [Perplexity AI](https://twitter.com/jmilldotdev/status/1600624362394091523)\n",
      "    - [Bing Chat](https://twitter.com/kliu128/status/1623472922374574080)\n",
      "- **Prompt**: Prompts are the questions that the model is asked to answer. They are usually a single\n",
      "  sentence or a short paragraph.\n",
      "  - **Notable Examples**:\n",
      "    - [ShareGPT](https://sharegpt.com/)\n",
      "    - [OpenAI Examples](https://platform.openai.com/examples/)\n",
      "- **Context**: Context is the memory that the model uses to generate its response. It usually\n",
      "  consists of the Preamble and some previous messages (or summaries them), and older messages are\n",
      "  eventually replaced by newer ones as the conversation progresses and tokens becomes more limited.\n",
      "- **Prefixes**: Prefixes are prepended to a Prompt and are generally used to include a Start Word\n",
      "  that identifies the start of a Prompt and a Handle that identifies the speaker.\n",
      "- **Suffixes**: Suffixes are appended to a Prompt and are generally used to include a Stop Word that\n",
      "  identifies the end of a Prompt.\n",
      "- **Handles**: Handles are short identifiers used to differentiate between speakers in the Context.\n",
      "  By default this plugin uses `You:` to represent the Human and `AI:` to represent the AI model. You\n",
      "  can change these prefixes in the AI Research Assistant settings.\n",
      "- **Memories**: When [Memories](#memories) are enabled, the Context is generated from the Preamble\n",
      "  and a defined number of previous messages. If the experimental Memory Manager is enabled, you can\n",
      "  edit the Context in real time for each Prompt by defining memory staties for previous messages.\n",
      "\n",
      "## Memories\n",
      "\n",
      "Memories give your Conversation context and allow the language model to use what's already been said\n",
      "to inform future responses.\n",
      "\n",
      "There are four different [Memory States](#memory-states), and each message can have one of them:\n",
      "\n",
      "You can manage the Memories of a Conversation in real time by clicking on the Memory State button\n",
      "next to each message bubble and choosing a new state for that memory.\n",
      "\n",
      "### Memory States\n",
      "\n",
      "- **Default**: By default, messages are **somtimes** remembered (in reverse chronological order),\n",
      "  the most recent Memories are most likely to be remembered and older Memories are the first to be\n",
      "  excluded frm the Conversation's Context. These Memories follow a basic First In First Out (FIFO)\n",
      "  queue.\n",
      "- **Core Memory**: Core Memories are **always** included in the Conversation's Context. They are\n",
      "  usually the most important messages in the conversation and are used to provide consistent Context\n",
      "  for the model. They do not count towards the maximum number of memories for a Conversation\n",
      "  configured in the plugin's settings.\n",
      "- **Remembered**: Remembered Memories **often** included in the Conversation's Context, until your\n",
      "  Conversation reaches the maximum number of memories configured in the plugin's settings. They are\n",
      "  usually more important messages, but not as important as Core Memories. They do count towards the\n",
      "  maximum number of memories, and are accessed in reverse chronological order. If the maximum number\n",
      "  of memories is reached, the oldest Remembered Memories will not be included in the Context.\n",
      "- **Forgotten**: Forgotten Memories are **never** included in the Conversation's Context. You can\n",
      "  forget a Memory at any time during the Conversation, and also restore Forgotten Memories to\n",
      "  another Memory State.\n",
      "---\n",
      "DEBUG:root:  create_time: 1693281218.583268\n",
      "DEBUG:root:  end_turn: None\n",
      "DEBUG:root:  id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:  metadata: ChatMessageMetadata(is_complete=None, message_type=None, model_slug=None, finish_details=None, timestamp_='absolute', parent_id=None)\n",
      "DEBUG:root:  recipient: all\n",
      "DEBUG:root:  status: finished_successfully\n",
      "DEBUG:root:  update_time: None\n",
      "DEBUG:root:  weight: 1.0\n",
      "DEBUG:root:MessageRecord.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  children: ['aaa24f97-0b99-4e65-be87-ef5225236595']\n",
      "DEBUG:root:  id: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:  message:\n",
      "DEBUG:root:    author:\n",
      "DEBUG:root:      metadata:\n",
      "DEBUG:root:      name: None\n",
      "DEBUG:root:      role: assistant\n",
      "DEBUG:root:    content:\n",
      "DEBUG:root:      content_type: text\n",
      "DEBUG:root:      parts: [\"Based on the information provided and the feature you want to request, here's a feature request draft for the Obsidian AI Research Assistant plugin:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nYou can use this draft as a starting point and customize it further based on your specific needs before submitting it as a feature request.\"]\n",
      "DEBUG:root:    create_time: 1693281237.862037\n",
      "DEBUG:root:    end_turn: True\n",
      "DEBUG:root:    id: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:    metadata:\n",
      "DEBUG:root:      finish_details:\n",
      "DEBUG:root:        stop_tokens: [100260]\n",
      "DEBUG:root:        type: stop\n",
      "DEBUG:root:      is_complete: True\n",
      "DEBUG:root:      message_type: None\n",
      "DEBUG:root:      model_slug: gpt-4-code-interpreter\n",
      "DEBUG:root:      parent_id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:      timestamp_: absolute\n",
      "DEBUG:root:    recipient: all\n",
      "DEBUG:root:    status: finished_successfully\n",
      "DEBUG:root:    update_time: None\n",
      "DEBUG:root:    weight: 1.0\n",
      "DEBUG:root:  parent: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:Author.from_dict got: {'role': 'assistant', 'name': None, 'metadata': {}}, which is of type: dict\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got: {'finish_details': {'type': 'stop', 'stop_tokens': [100260]}, 'is_complete': True, 'message_type': None, 'model_slug': 'gpt-4-code-interpreter', 'parent_id': 'aaa2bd91-80e5-456c-8f73-6f0cf6adcd15', 'timestamp_': 'absolute'}, which is of type: dict\n",
      "DEBUG:root:FinishDetails.from_dict got: {'type': 'stop', 'stop_tokens': [100260]}, which is of type: dict\n",
      "DEBUG:root:Content.from_dict got: {'content_type': 'text', 'parts': [\"Based on the information provided and the feature you want to request, here's a feature request draft for the Obsidian AI Research Assistant plugin:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nYou can use this draft as a starting point and customize it further based on your specific needs before submitting it as a feature request.\"]}, which is of type: dict\n",
      "DEBUG:root:Content.from_dict got a dict with no metadata. Setting to empty dict.\n",
      "DEBUG:root:ChatMessage.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  author: Author(role=<RoleType.ASSISTANT: 'assistant'>, name=None, metadata={})\n",
      "DEBUG:root:  content: Based on the information provided and the feature you want to request, here's a feature request draft for the Obsidian AI Research Assistant plugin:\n",
      "\n",
      "```\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "I frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "I would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "An alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\n",
      "\n",
      "**Additional context**\n",
      "Integrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\n",
      "```\n",
      "\n",
      "You can use this draft as a starting point and customize it further based on your specific needs before submitting it as a feature request.\n",
      "DEBUG:root:  create_time: 1693281237.862037\n",
      "DEBUG:root:  end_turn: True\n",
      "DEBUG:root:  id: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:  metadata: ChatMessageMetadata(is_complete=True, message_type=None, model_slug='gpt-4-code-interpreter', finish_details=FinishDetails(type='stop', stop_tokens=[100260]), timestamp_='absolute', parent_id='aaa2bd91-80e5-456c-8f73-6f0cf6adcd15')\n",
      "DEBUG:root:  recipient: all\n",
      "DEBUG:root:  status: finished_successfully\n",
      "DEBUG:root:  update_time: None\n",
      "DEBUG:root:  weight: 1.0\n",
      "DEBUG:root:MessageRecord.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  children: ['29bde5f1-d6d7-416e-8fb7-b2bf6c5db465']\n",
      "DEBUG:root:  id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:  message:\n",
      "DEBUG:root:    author:\n",
      "DEBUG:root:      metadata:\n",
      "DEBUG:root:      name: None\n",
      "DEBUG:root:      role: user\n",
      "DEBUG:root:    content:\n",
      "DEBUG:root:      content_type: text\n",
      "DEBUG:root:      parts: ['Could you please add a minor reference to https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json, which is as complete of a description of ? ']\n",
      "DEBUG:root:    create_time: 1693281722.670299\n",
      "DEBUG:root:    end_turn: None\n",
      "DEBUG:root:    id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:    metadata:\n",
      "DEBUG:root:      message_type: None\n",
      "DEBUG:root:      timestamp_: absolute\n",
      "DEBUG:root:    recipient: all\n",
      "DEBUG:root:    status: finished_successfully\n",
      "DEBUG:root:    update_time: None\n",
      "DEBUG:root:    weight: 1.0\n",
      "DEBUG:root:  parent: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Import ChatGPT JSON Format', 'create_time': 1693281218.581128, 'update_time': 1693281747.0, 'mapping': {'55308928-5076-4c81-bbfd-4bc504b237eb': {'id': '55308928-5076-4c81-bbfd-4bc504b237eb', 'message': {'id': '55308928-5076-4c81-bbfd-4bc504b237eb', 'author': {'role': 'system', 'name': None, 'metadata': {}}, 'create_time': None, 'update_time': None, 'content': {'content_type': 'text', 'parts': ['']}, 'status': 'finished_successfully', 'end_turn': True, 'weight': 0.0, 'metadata': {}, 'recipient': 'all'}, 'parent': 'aaa13816-46a6-4b26-ac95-dc204c5220ef', 'children': ['aaa2bd91-80e5-456c-8f73-6f0cf6adcd15']}, 'aaa13816-46a6-4b26-ac95-dc204c5220ef': {'id': 'aaa13816-46a6-4b26-ac95-dc204c5220ef', 'message': None, 'parent': None, 'children': ['55308928-5076-4c81-bbfd-4bc504b237eb']}, 'aaa2bd91-80e5-456c-8f73-6f0cf6adcd15': {'id': 'aaa2bd91-80e5-456c-8f73-6f0cf6adcd15', 'message': {'id': 'aaa2bd91-80e5-456c-8f73-6f0cf6adcd15', 'author': {'role': 'user', 'name': None, 'metadata': {}}, 'create_time': 1693281218.583268, 'update_time': None, 'content': {'content_type': 'text', 'parts': [\"I need help writing a feature request for a GitHub project I'm really excited about. The project is called Obsidian AI Research Assistant and is a plugin for Obsidian. I'm include the default text from the feature request below, delineated with triple backticks. I've included the project README.md below and delineated it with triple hyphens. \\n\\nThe feature I want to request is to take a ChatGPT data export JSON file and transform it into the AI Research Assistant's chat history format so that I can view all the chats I've exported. \\n\\nFeature request default text: ```\\n**Is your feature request related to a problem? Please describe.** A clear and concise description\\nof what the problem is. Ex. I'm always frustrated when [...]\\n\\n**Describe the solution you'd like** A clear and concise description of what you want to happen.\\n\\n**Describe alternatives you've considered** A clear and concise description of any alternative\\nsolutions or features you've considered.\\n\\n**Additional context** Add any other context or screenshots about the feature request here.\\n```\\n\\nREADME.md file:\\n---\\n# Obsidian AI Research Assistant\\n\\n![Forgetting a previous message](./docs/assets/forget-memory.png)\\n\\n> Build better Prompts and AI integrations with this advanced research tool for Prompt Engineering with the ChatGPT API.\\n\\n**Note**: This plugin is still in development and is not considered fully stable yet. **This is Beta\\nsoftware** and may contain bugs and unexpected behaviors. Please report\\n[Issues](https:/github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues) you find and feel\\nencouraged to [contribute](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/blob/main/docs/CONTRIBUTING.md) to the project.\\n\\n**Table of Contents**:\\n\\n- [Obsidian AI Research Assistant](#obsidian-ai-research-assistant)\\n  - [Summary](#summary)\\n  - [Features](#features)\\n  - [Roadmap](#roadmap)\\n    - [Probably Not on the Roadmap](#probably-not-on-the-roadmap)\\n  - [Installation](#installation)\\n    - [Other Installation Methods](#other-installation-methods)\\n    - [BRAT Installation](#brat-installation)\\n    - [Manual Installation](#manual-installation)\\n  - [Currently Supported Models](#currently-supported-models)\\n    - [Upcoming Model Support](#upcoming-model-support)\\n  - [Naming Conventions](#naming-conventions)\\n  - [Memories](#memories)\\n    - [Memory States](#memory-states)\\n\\n## Summary\\n\\nThis plugin integrates tools for Prompt Engineering and researching AI tools and language models\\nlike OpenAI's ChatGPT into Obsidian.\\n\\n![Basic interface and conversation summary](./docs/assets/basic-view.png)\\n\\nIt allows you to manually or automatically save your conversations with AI models, and then use\\nObsidian's powerful search and tagging features to organize and analyze them.\\n\\n![View the Preamble for the conversation](./docs/assets/preamble-summary.png)\\n\\nThe embedded Memory Manager allows you to edit which messages are included in the conversation's context, and you can even switch models mid-conversation to compare the output of various models to the same inputs.\\n\\n![Marking a previous message as a Core Memory](./docs/assets/core-memory.png)\\n\\nIt allows you to review, search, tag, and link your conversations in Obsidian, and view a summary of\\nthe conversation details its raw inputs and outputs.\\n\\n![View the conversational exchange](./docs/assets/conversation-view.png)\\n\\n![View the actual input sent to the API for requests](./docs/assets/raw-user-input.png)\\n\\n![View the raw JSON from the API for responses](./docs/assets/raw-response.png)\\n\\n## Features\\n\\n- Save (automatically or manually) conversations with AI models to Obsidian notes\\n  - Conversations become searchable, taggagle, and linkable in Obsidian\\n- Live edit which previous messages are used as Context for each Prompt\\n  - Mark a message as a **Core Memory** to make sure it's always in the Context\\n  - Mark a message as **Forgotten** to make sure it's never in the Context\\n  - Mark a message as **Remembered** to prioritize it's inclusion in the Context\\n  - By default the most recent messages are more likely to be remembered and older messages are forgotten as the conversation nears the memory and token limits\\n- Live edit the Preamble that is used for each Conversation\\n- Live edit the Prompt that is used for each Conversation\\n- View the raw JSON from the API for each response\\n- View the prompt and context that was sent to the API for each request\\n\\n## Roadmap\\n\\n- **Token-aware Memories**: Memory will be constructed within the desired amount of tokens\\n- **Configurable Response Token Buffer**: Ensure that the API has enough tokens to respond to your prompts\\n- **Models & APIs**: [More models](#upcoming-model-support)\\n- **Resume Conversations** Load conversations from Obsidian notes to pick up where you left off\\n- **Annotate Conversations**: Incrementally update notes on save (instead of overwriting the whole note on each save)\\n- **Title Generator**: Automatically generate a title based on the conversation (like ChatGPT)\\n- **Conversation Presets**: Save and Load preset configurations of Prefix, Model, Preamble, Context, etc. to make it easier to explore different research threads\\n- **LangChain Support**: Integrate with [LangChain](https://langchain.readthedocs.io/) via [langchan.js](https://github.com/hwchase17/langchainjs)\\n- **Features You Want**: [Request a Feature](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BRequest%5D%3A+)\\n\\n### Probably Not on the Roadmap\\n\\n- **Mobile Support**: Use AI Research Assistant on your mobile devices, too\\n  - Due to how Obsidian plugins handle storage of things like OpenAI API keys, there currently isn't a safe way to provide user's with an option to securely store their API key in a way that can be synced to mobile devices, too.\\n  - On desktop devices, Obsidian uses [Electron](https://www.electronjs.org/) and this plugin relies on Electron's [`safeStorage()` API](https://www.electronjs.org/docs/latest/api/safe-storage) to store secrets like OpenAI API Keys.\\n\\n## Installation\\n\\nThis plugin is now avaialble in the Obsidian Community Plugins Directory, and can be installed directly from the Obsidian UI.\\n\\n1. Open Obsidian's Settings\\n2. Click on **Community Plugins**\\n3. Click on **Browse** to open the Community Plugins Directory\\n4. Search for `AI Research Assistant`\\n5. Click on the plugin to open its page\\n6. Click **Install**\\n7. Return to the **Community Plugins** settings in Obsidian\\n8. Toggle the switch to enable `AI Research Assistant`\\n9. Configure `AI Research Assistant` in Obsidian's Settings\\n10. Enter your OpenAI API Key in the plugin's settings\\n11. Click the **Save API Key** button to save your API Key to Obsidian's secure storage\\n12. Click on the `AI Research Assistant` icon in the left sidebar to open the plugin and start chatting\\n\\n### Other Installation Methods\\n\\nYou can also [install it manually](#manual-installation) or [install it via the Beta Reviewer's Auto-update Tool (BRAT)](#brat-installation), though the official Community Plugin Directory is the recommended installation method.\\n\\n### BRAT Installation\\n\\n1. Install [BRAT](https://github.com/TfTHacker/obsidian42-brat)\\n2. Click the `Add Beta Plugin` button in BRAT's settings\\n3. Enter this repository's URL: `https://github.com/InterwebAlchemy/obsidian-ai-research-assistant`\\n4. Enable the plugin in Obsidian's Community Plugins settings\\n5. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n### Manual Installation\\n\\n1. Download the latest\\n   [release](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/releases)\\n2. Unzip the release into your Obsidian vault's `plugins` folder\\n3. Enable the plugin in Obsidian's Community Plugins settings\\n4. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n## Currently Supported Models\\n\\n- [OpenAI](https://platform.openai.com/docs/models)\\n  - [`gpt-4`](https://platform.openai.com/docs/models/gpt-4)\\n  - [`gpt-3.5-turbo`](https://platform.openai.com/docs/models/gpt-3-5)\\n\\n### Upcoming Model Support\\n\\n- [OpenAI Legacy Completions API](https://platform.openai.com/docs/deprecations/instructgpt-models)\\n  - `gpt-3.5-turbo-instruct`\\n  - **Note**: This plugin was originally built when only the completions API was available and was built to support the `text-davinci-003` model, but with recent changes in the OpenAI API, it is now recommended to use the `gpt-3.5-turbo` model instead and `text-daivinci-003` is being deprecated, so that functionality has been temporarily disabled in this plugin.\\n- Llama2\\n- Anthropic Claude\\n\\n## Naming Conventions\\n\\nThis plugin uses the following naming conventions to refer to different pieces of a conversation and\\nmake sure that it is always clear what is being referred to:\\n\\n**Note**: This plugin makes a distinction between a `Prompt` and a `Preamble` because it can be used\\nto generate prompts for models without a Preamble and in that case it is harder to distinguish\\nbetween what you might be editing when you click on an `Edit Prompt` button.\\n\\n- **Conversation**: This is an ongoing exchange of messages between the Human and the AI.\\n- **Preamble**: This is the initial instructions that a language model recieves. It is usually a\\n  short description of the topic of the conversation, and is used to provide context on how the\\n  model should behave, what the models knows, and how it should respond.\\n  - **Notable Examples**:\\n    - [ChatGPT](https://twitter.com/goodside/status/1598253337400717313)\\n    - [Perplexity AI](https://twitter.com/jmilldotdev/status/1600624362394091523)\\n    - [Bing Chat](https://twitter.com/kliu128/status/1623472922374574080)\\n- **Prompt**: Prompts are the questions that the model is asked to answer. They are usually a single\\n  sentence or a short paragraph.\\n  - **Notable Examples**:\\n    - [ShareGPT](https://sharegpt.com/)\\n    - [OpenAI Examples](https://platform.openai.com/examples/)\\n- **Context**: Context is the memory that the model uses to generate its response. It usually\\n  consists of the Preamble and some previous messages (or summaries them), and older messages are\\n  eventually replaced by newer ones as the conversation progresses and tokens becomes more limited.\\n- **Prefixes**: Prefixes are prepended to a Prompt and are generally used to include a Start Word\\n  that identifies the start of a Prompt and a Handle that identifies the speaker.\\n- **Suffixes**: Suffixes are appended to a Prompt and are generally used to include a Stop Word that\\n  identifies the end of a Prompt.\\n- **Handles**: Handles are short identifiers used to differentiate between speakers in the Context.\\n  By default this plugin uses `You:` to represent the Human and `AI:` to represent the AI model. You\\n  can change these prefixes in the AI Research Assistant settings.\\n- **Memories**: When [Memories](#memories) are enabled, the Context is generated from the Preamble\\n  and a defined number of previous messages. If the experimental Memory Manager is enabled, you can\\n  edit the Context in real time for each Prompt by defining memory staties for previous messages.\\n\\n## Memories\\n\\nMemories give your Conversation context and allow the language model to use what's already been said\\nto inform future responses.\\n\\nThere are four different [Memory States](#memory-states), and each message can have one of them:\\n\\nYou can manage the Memories of a Conversation in real time by clicking on the Memory State button\\nnext to each message bubble and choosing a new state for that memory.\\n\\n### Memory States\\n\\n- **Default**: By default, messages are **somtimes** remembered (in reverse chronological order),\\n  the most recent Memories are most likely to be remembered and older Memories are the first to be\\n  excluded frm the Conversation's Context. These Memories follow a basic First In First Out (FIFO)\\n  queue.\\n- **Core Memory**: Core Memories are **always** included in the Conversation's Context. They are\\n  usually the most important messages in the conversation and are used to provide consistent Context\\n  for the model. They do not count towards the maximum number of memories for a Conversation\\n  configured in the plugin's settings.\\n- **Remembered**: Remembered Memories **often** included in the Conversation's Context, until your\\n  Conversation reaches the maximum number of memories configured in the plugin's settings. They are\\n  usually more important messages, but not as important as Core Memories. They do count towards the\\n  maximum number of memories, and are accessed in reverse chronological order. If the maximum number\\n  of memories is reached, the oldest Remembered Memories will not be included in the Context.\\n- **Forgotten**: Forgotten Memories are **never** included in the Conversation's Context. You can\\n  forget a Memory at any time during the Conversation, and also restore Forgotten Memories to\\n  another Memory State.\\n---\"]}, 'status': 'finished_successfully', 'end_turn': None, 'weight': 1.0, 'metadata': {'timestamp_': 'absolute', 'message_type': None}, 'recipient': 'all'}, 'parent': '55308928-5076-4c81-bbfd-4bc504b237eb', 'children': ['6d02e2fb-8fc8-4f79-897c-ce1a736210b6']}, '6d02e2fb-8fc8-4f79-897c-ce1a736210b6': {'id': '6d02e2fb-8fc8-4f79-897c-ce1a736210b6', 'message': {'id': '6d02e2fb-8fc8-4f79-897c-ce1a736210b6', 'author': {'role': 'assistant', 'name': None, 'metadata': {}}, 'create_time': 1693281237.862037, 'update_time': None, 'content': {'content_type': 'text', 'parts': [\"Based on the information provided and the feature you want to request, here's a feature request draft for the Obsidian AI Research Assistant plugin:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nYou can use this draft as a starting point and customize it further based on your specific needs before submitting it as a feature request.\"]}, 'status': 'finished_successfully', 'end_turn': True, 'weight': 1.0, 'metadata': {'finish_details': {'type': 'stop', 'stop_tokens': [100260]}, 'is_complete': True, 'message_type': None, 'model_slug': 'gpt-4-code-interpreter', 'parent_id': 'aaa2bd91-80e5-456c-8f73-6f0cf6adcd15', 'timestamp_': 'absolute'}, 'recipient': 'all'}, 'parent': 'aaa2bd91-80e5-456c-8f73-6f0cf6adcd15', 'children': ['aaa24f97-0b99-4e65-be87-ef5225236595']}, 'aaa24f97-0b99-4e65-be87-ef5225236595': {'id': 'aaa24f97-0b99-4e65-be87-ef5225236595', 'message': {'id': 'aaa24f97-0b99-4e65-be87-ef5225236595', 'author': {'role': 'user', 'name': None, 'metadata': {}}, 'create_time': 1693281722.670299, 'update_time': None, 'content': {'content_type': 'text', 'parts': ['Could you please add a minor reference to https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json, which is as complete of a description of ? ']}, 'status': 'finished_successfully', 'end_turn': None, 'weight': 1.0, 'metadata': {'timestamp_': 'absolute', 'message_type': None}, 'recipient': 'all'}, 'parent': '6d02e2fb-8fc8-4f79-897c-ce1a736210b6', 'children': ['29bde5f1-d6d7-416e-8fb7-b2bf6c5db465']}, '29bde5f1-d6d7-416e-8fb7-b2bf6c5db465': {'id': '29bde5f1-d6d7-416e-8fb7-b2bf6c5db465', 'message': {'id': '29bde5f1-d6d7-416e-8fb7-b2bf6c5db465', 'author': {'role': 'assistant', 'name': None, 'metadata': {}}, 'create_time': 1693281747.092052, 'update_time': None, 'content': {'content_type': 'text', 'parts': [\"Of course! Incorporating a reference to the JSON schema for ChatGPT chat exports will provide clarity and context for the developers of the Obsidian AI Research Assistant. Here's the updated feature request draft:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\nFor a detailed structure of the ChatGPT JSON data export, you can refer to this [JSON schema](https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json).\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nThis reference should help the developers understand the structure and specifics of the ChatGPT data export format, facilitating the development process for the desired feature.\"]}, 'status': 'finished_successfully', 'end_turn': True, 'weight': 1.0, 'metadata': {'finish_details': {'type': 'stop', 'stop_tokens': [100260]}, 'is_complete': True, 'message_type': None, 'model_slug': 'gpt-4-code-interpreter', 'parent_id': 'aaa24f97-0b99-4e65-be87-ef5225236595', 'timestamp_': 'absolute'}, 'recipient': 'all'}, 'parent': 'aaa24f97-0b99-4e65-be87-ef5225236595', 'children': []}}, 'moderation_results': [], 'current_node': '29bde5f1-d6d7-416e-8fb7-b2bf6c5db465', 'plugin_ids': None, 'conversation_id': '930bb6fd-00e1-4abc-8ab9-5f77c39bbb28', 'conversation_template_id': None, 'id': '930bb6fd-00e1-4abc-8ab9-5f77c39bbb28'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Author.from_dict got: {'role': 'user', 'name': None, 'metadata': {}}, which is of type: dict\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got: {'timestamp_': 'absolute', 'message_type': None}, which is of type: dict\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got no `finish_details`. Setting to None.\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got no `model_slug`. Setting to None.\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got no `is_complete`. Setting to None.\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got no `parent_id`. Setting to None.\n",
      "DEBUG:root:Content.from_dict got: {'content_type': 'text', 'parts': ['Could you please add a minor reference to https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json, which is as complete of a description of ? ']}, which is of type: dict\n",
      "DEBUG:root:Content.from_dict got a dict with no metadata. Setting to empty dict.\n",
      "DEBUG:root:ChatMessage.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  author: Author(role=<RoleType.USER: 'user'>, name=None, metadata={})\n",
      "DEBUG:root:  content: Could you please add a minor reference to https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json, which is as complete of a description of ? \n",
      "DEBUG:root:  create_time: 1693281722.670299\n",
      "DEBUG:root:  end_turn: None\n",
      "DEBUG:root:  id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:  metadata: ChatMessageMetadata(is_complete=None, message_type=None, model_slug=None, finish_details=None, timestamp_='absolute', parent_id=None)\n",
      "DEBUG:root:  recipient: all\n",
      "DEBUG:root:  status: finished_successfully\n",
      "DEBUG:root:  update_time: None\n",
      "DEBUG:root:  weight: 1.0\n",
      "DEBUG:root:MessageRecord.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  children: []\n",
      "DEBUG:root:  id: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:  message:\n",
      "DEBUG:root:    author:\n",
      "DEBUG:root:      metadata:\n",
      "DEBUG:root:      name: None\n",
      "DEBUG:root:      role: assistant\n",
      "DEBUG:root:    content:\n",
      "DEBUG:root:      content_type: text\n",
      "DEBUG:root:      parts: [\"Of course! Incorporating a reference to the JSON schema for ChatGPT chat exports will provide clarity and context for the developers of the Obsidian AI Research Assistant. Here's the updated feature request draft:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\nFor a detailed structure of the ChatGPT JSON data export, you can refer to this [JSON schema](https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json).\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nThis reference should help the developers understand the structure and specifics of the ChatGPT data export format, facilitating the development process for the desired feature.\"]\n",
      "DEBUG:root:    create_time: 1693281747.092052\n",
      "DEBUG:root:    end_turn: True\n",
      "DEBUG:root:    id: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:    metadata:\n",
      "DEBUG:root:      finish_details:\n",
      "DEBUG:root:        stop_tokens: [100260]\n",
      "DEBUG:root:        type: stop\n",
      "DEBUG:root:      is_complete: True\n",
      "DEBUG:root:      message_type: None\n",
      "DEBUG:root:      model_slug: gpt-4-code-interpreter\n",
      "DEBUG:root:      parent_id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:      timestamp_: absolute\n",
      "DEBUG:root:    recipient: all\n",
      "DEBUG:root:    status: finished_successfully\n",
      "DEBUG:root:    update_time: None\n",
      "DEBUG:root:    weight: 1.0\n",
      "DEBUG:root:  parent: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:Author.from_dict got: {'role': 'assistant', 'name': None, 'metadata': {}}, which is of type: dict\n",
      "DEBUG:root:ChatMessageMetadata.from_dict got: {'finish_details': {'type': 'stop', 'stop_tokens': [100260]}, 'is_complete': True, 'message_type': None, 'model_slug': 'gpt-4-code-interpreter', 'parent_id': 'aaa24f97-0b99-4e65-be87-ef5225236595', 'timestamp_': 'absolute'}, which is of type: dict\n",
      "DEBUG:root:FinishDetails.from_dict got: {'type': 'stop', 'stop_tokens': [100260]}, which is of type: dict\n",
      "DEBUG:root:Content.from_dict got: {'content_type': 'text', 'parts': [\"Of course! Incorporating a reference to the JSON schema for ChatGPT chat exports will provide clarity and context for the developers of the Obsidian AI Research Assistant. Here's the updated feature request draft:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\nFor a detailed structure of the ChatGPT JSON data export, you can refer to this [JSON schema](https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json).\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nThis reference should help the developers understand the structure and specifics of the ChatGPT data export format, facilitating the development process for the desired feature.\"]}, which is of type: dict\n",
      "DEBUG:root:Content.from_dict got a dict with no metadata. Setting to empty dict.\n",
      "DEBUG:root:ChatMessage.from_dict got dictionary of type: dict:\n",
      "DEBUG:root:  author: Author(role=<RoleType.ASSISTANT: 'assistant'>, name=None, metadata={})\n",
      "DEBUG:root:  content: Of course! Incorporating a reference to the JSON schema for ChatGPT chat exports will provide clarity and context for the developers of the Obsidian AI Research Assistant. Here's the updated feature request draft:\n",
      "\n",
      "```\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "I frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "I would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\n",
      "\n",
      "For a detailed structure of the ChatGPT JSON data export, you can refer to this [JSON schema](https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json).\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "An alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\n",
      "\n",
      "**Additional context**\n",
      "Integrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\n",
      "```\n",
      "\n",
      "This reference should help the developers understand the structure and specifics of the ChatGPT data export format, facilitating the development process for the desired feature.\n",
      "DEBUG:root:  create_time: 1693281747.092052\n",
      "DEBUG:root:  end_turn: True\n",
      "DEBUG:root:  id: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:  metadata: ChatMessageMetadata(is_complete=True, message_type=None, model_slug='gpt-4-code-interpreter', finish_details=FinishDetails(type='stop', stop_tokens=[100260]), timestamp_='absolute', parent_id='aaa24f97-0b99-4e65-be87-ef5225236595')\n",
      "DEBUG:root:  recipient: all\n",
      "DEBUG:root:  status: finished_successfully\n",
      "DEBUG:root:  update_time: None\n",
      "DEBUG:root:  weight: 1.0\n",
      "DEBUG:root:================================================================================\n",
      "DEBUG:root:Conversation Object to Dict:\n",
      "DEBUG:root:  conversation_id: 930bb6fd-00e1-4abc-8ab9-5f77c39bbb28\n",
      "DEBUG:root:  conversation_template_id: None\n",
      "DEBUG:root:  create_time: 1693281218.581128\n",
      "DEBUG:root:  current_node: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:  id: 930bb6fd-00e1-4abc-8ab9-5f77c39bbb28\n",
      "DEBUG:root:  mapping:\n",
      "DEBUG:root:    29bde5f1-d6d7-416e-8fb7-b2bf6c5db465:\n",
      "DEBUG:root:      children: []\n",
      "DEBUG:root:      id: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: RoleType.ASSISTANT\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: ContentType.TEXT\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          parts: [\"Of course! Incorporating a reference to the JSON schema for ChatGPT chat exports will provide clarity and context for the developers of the Obsidian AI Research Assistant. Here's the updated feature request draft:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\nFor a detailed structure of the ChatGPT JSON data export, you can refer to this [JSON schema](https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json).\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nThis reference should help the developers understand the structure and specifics of the ChatGPT data export format, facilitating the development process for the desired feature.\"]\n",
      "DEBUG:root:        create_time: 1693281747.092052\n",
      "DEBUG:root:        end_turn: True\n",
      "DEBUG:root:        id: 29bde5f1-d6d7-416e-8fb7-b2bf6c5db465\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:          finish_details:\n",
      "DEBUG:root:            stop_tokens: [100260]\n",
      "DEBUG:root:            type: stop\n",
      "DEBUG:root:          is_complete: True\n",
      "DEBUG:root:          message_type: None\n",
      "DEBUG:root:          model_slug: gpt-4-code-interpreter\n",
      "DEBUG:root:          parent_id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:          timestamp_: absolute\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 1.0\n",
      "DEBUG:root:      parent: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:    55308928-5076-4c81-bbfd-4bc504b237eb:\n",
      "DEBUG:root:      children: ['aaa2bd91-80e5-456c-8f73-6f0cf6adcd15']\n",
      "DEBUG:root:      id: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: RoleType.SYSTEM\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: ContentType.TEXT\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          parts: ['']\n",
      "DEBUG:root:        create_time: None\n",
      "DEBUG:root:        end_turn: True\n",
      "DEBUG:root:        id: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 0.0\n",
      "DEBUG:root:      parent: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:root:    6d02e2fb-8fc8-4f79-897c-ce1a736210b6:\n",
      "DEBUG:root:      children: ['aaa24f97-0b99-4e65-be87-ef5225236595']\n",
      "DEBUG:root:      id: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: RoleType.ASSISTANT\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: ContentType.TEXT\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          parts: [\"Based on the information provided and the feature you want to request, here's a feature request draft for the Obsidian AI Research Assistant plugin:\\n\\n```\\n**Is your feature request related to a problem? Please describe.**\\nI frequently use ChatGPT and export my chat sessions as JSON files. Currently, there's no easy way to integrate these exported chat histories into the Obsidian AI Research Assistant's chat history format.\\n\\n**Describe the solution you'd like**\\nI would like a feature within the Obsidian AI Research Assistant that allows users to import ChatGPT data export JSON files. This feature should convert the ChatGPT chat format into the AI Research Assistant's chat history format, enabling users to view and analyze all their past ChatGPT sessions seamlessly within Obsidian.\\n\\n**Describe alternatives you've considered**\\nAn alternative would be to manually convert the ChatGPT JSON data into the format supported by the AI Research Assistant. However, this can be time-consuming and prone to errors, especially for long chat histories.\\n\\n**Additional context**\\nIntegrating this feature will significantly enhance the user experience for those who actively use both ChatGPT and the Obsidian AI Research Assistant, making it easier to manage and review past interactions.\\n```\\n\\nYou can use this draft as a starting point and customize it further based on your specific needs before submitting it as a feature request.\"]\n",
      "DEBUG:root:        create_time: 1693281237.862037\n",
      "DEBUG:root:        end_turn: True\n",
      "DEBUG:root:        id: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:          finish_details:\n",
      "DEBUG:root:            stop_tokens: [100260]\n",
      "DEBUG:root:            type: stop\n",
      "DEBUG:root:          is_complete: True\n",
      "DEBUG:root:          message_type: None\n",
      "DEBUG:root:          model_slug: gpt-4-code-interpreter\n",
      "DEBUG:root:          parent_id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:          timestamp_: absolute\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 1.0\n",
      "DEBUG:root:      parent: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:    aaa13816-46a6-4b26-ac95-dc204c5220ef:\n",
      "DEBUG:root:      children: ['55308928-5076-4c81-bbfd-4bc504b237eb']\n",
      "DEBUG:root:      id: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:root:      message: None\n",
      "DEBUG:root:      parent: None\n",
      "DEBUG:root:    aaa24f97-0b99-4e65-be87-ef5225236595:\n",
      "DEBUG:root:      children: ['29bde5f1-d6d7-416e-8fb7-b2bf6c5db465']\n",
      "DEBUG:root:      id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: RoleType.USER\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: ContentType.TEXT\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          parts: ['Could you please add a minor reference to https://github.com/blakejwc/chatgpt-as-obsidian-vault/blob/main/resources/conversations_jsonschema.json, which is as complete of a description of ? ']\n",
      "DEBUG:root:        create_time: 1693281722.670299\n",
      "DEBUG:root:        end_turn: None\n",
      "DEBUG:root:        id: aaa24f97-0b99-4e65-be87-ef5225236595\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:          finish_details: None\n",
      "DEBUG:root:          is_complete: None\n",
      "DEBUG:root:          message_type: None\n",
      "DEBUG:root:          model_slug: None\n",
      "DEBUG:root:          parent_id: None\n",
      "DEBUG:root:          timestamp_: absolute\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 1.0\n",
      "DEBUG:root:      parent: 6d02e2fb-8fc8-4f79-897c-ce1a736210b6\n",
      "DEBUG:root:    aaa2bd91-80e5-456c-8f73-6f0cf6adcd15:\n",
      "DEBUG:root:      children: ['6d02e2fb-8fc8-4f79-897c-ce1a736210b6']\n",
      "DEBUG:root:      id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:      message:\n",
      "DEBUG:root:        author:\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          name: None\n",
      "DEBUG:root:          role: RoleType.USER\n",
      "DEBUG:root:        content:\n",
      "DEBUG:root:          content_type: ContentType.TEXT\n",
      "DEBUG:root:          metadata:\n",
      "DEBUG:root:          parts: [\"I need help writing a feature request for a GitHub project I'm really excited about. The project is called Obsidian AI Research Assistant and is a plugin for Obsidian. I'm include the default text from the feature request below, delineated with triple backticks. I've included the project README.md below and delineated it with triple hyphens. \\n\\nThe feature I want to request is to take a ChatGPT data export JSON file and transform it into the AI Research Assistant's chat history format so that I can view all the chats I've exported. \\n\\nFeature request default text: ```\\n**Is your feature request related to a problem? Please describe.** A clear and concise description\\nof what the problem is. Ex. I'm always frustrated when [...]\\n\\n**Describe the solution you'd like** A clear and concise description of what you want to happen.\\n\\n**Describe alternatives you've considered** A clear and concise description of any alternative\\nsolutions or features you've considered.\\n\\n**Additional context** Add any other context or screenshots about the feature request here.\\n```\\n\\nREADME.md file:\\n---\\n# Obsidian AI Research Assistant\\n\\n![Forgetting a previous message](./docs/assets/forget-memory.png)\\n\\n> Build better Prompts and AI integrations with this advanced research tool for Prompt Engineering with the ChatGPT API.\\n\\n**Note**: This plugin is still in development and is not considered fully stable yet. **This is Beta\\nsoftware** and may contain bugs and unexpected behaviors. Please report\\n[Issues](https:/github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues) you find and feel\\nencouraged to [contribute](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/blob/main/docs/CONTRIBUTING.md) to the project.\\n\\n**Table of Contents**:\\n\\n- [Obsidian AI Research Assistant](#obsidian-ai-research-assistant)\\n  - [Summary](#summary)\\n  - [Features](#features)\\n  - [Roadmap](#roadmap)\\n    - [Probably Not on the Roadmap](#probably-not-on-the-roadmap)\\n  - [Installation](#installation)\\n    - [Other Installation Methods](#other-installation-methods)\\n    - [BRAT Installation](#brat-installation)\\n    - [Manual Installation](#manual-installation)\\n  - [Currently Supported Models](#currently-supported-models)\\n    - [Upcoming Model Support](#upcoming-model-support)\\n  - [Naming Conventions](#naming-conventions)\\n  - [Memories](#memories)\\n    - [Memory States](#memory-states)\\n\\n## Summary\\n\\nThis plugin integrates tools for Prompt Engineering and researching AI tools and language models\\nlike OpenAI's ChatGPT into Obsidian.\\n\\n![Basic interface and conversation summary](./docs/assets/basic-view.png)\\n\\nIt allows you to manually or automatically save your conversations with AI models, and then use\\nObsidian's powerful search and tagging features to organize and analyze them.\\n\\n![View the Preamble for the conversation](./docs/assets/preamble-summary.png)\\n\\nThe embedded Memory Manager allows you to edit which messages are included in the conversation's context, and you can even switch models mid-conversation to compare the output of various models to the same inputs.\\n\\n![Marking a previous message as a Core Memory](./docs/assets/core-memory.png)\\n\\nIt allows you to review, search, tag, and link your conversations in Obsidian, and view a summary of\\nthe conversation details its raw inputs and outputs.\\n\\n![View the conversational exchange](./docs/assets/conversation-view.png)\\n\\n![View the actual input sent to the API for requests](./docs/assets/raw-user-input.png)\\n\\n![View the raw JSON from the API for responses](./docs/assets/raw-response.png)\\n\\n## Features\\n\\n- Save (automatically or manually) conversations with AI models to Obsidian notes\\n  - Conversations become searchable, taggagle, and linkable in Obsidian\\n- Live edit which previous messages are used as Context for each Prompt\\n  - Mark a message as a **Core Memory** to make sure it's always in the Context\\n  - Mark a message as **Forgotten** to make sure it's never in the Context\\n  - Mark a message as **Remembered** to prioritize it's inclusion in the Context\\n  - By default the most recent messages are more likely to be remembered and older messages are forgotten as the conversation nears the memory and token limits\\n- Live edit the Preamble that is used for each Conversation\\n- Live edit the Prompt that is used for each Conversation\\n- View the raw JSON from the API for each response\\n- View the prompt and context that was sent to the API for each request\\n\\n## Roadmap\\n\\n- **Token-aware Memories**: Memory will be constructed within the desired amount of tokens\\n- **Configurable Response Token Buffer**: Ensure that the API has enough tokens to respond to your prompts\\n- **Models & APIs**: [More models](#upcoming-model-support)\\n- **Resume Conversations** Load conversations from Obsidian notes to pick up where you left off\\n- **Annotate Conversations**: Incrementally update notes on save (instead of overwriting the whole note on each save)\\n- **Title Generator**: Automatically generate a title based on the conversation (like ChatGPT)\\n- **Conversation Presets**: Save and Load preset configurations of Prefix, Model, Preamble, Context, etc. to make it easier to explore different research threads\\n- **LangChain Support**: Integrate with [LangChain](https://langchain.readthedocs.io/) via [langchan.js](https://github.com/hwchase17/langchainjs)\\n- **Features You Want**: [Request a Feature](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BRequest%5D%3A+)\\n\\n### Probably Not on the Roadmap\\n\\n- **Mobile Support**: Use AI Research Assistant on your mobile devices, too\\n  - Due to how Obsidian plugins handle storage of things like OpenAI API keys, there currently isn't a safe way to provide user's with an option to securely store their API key in a way that can be synced to mobile devices, too.\\n  - On desktop devices, Obsidian uses [Electron](https://www.electronjs.org/) and this plugin relies on Electron's [`safeStorage()` API](https://www.electronjs.org/docs/latest/api/safe-storage) to store secrets like OpenAI API Keys.\\n\\n## Installation\\n\\nThis plugin is now avaialble in the Obsidian Community Plugins Directory, and can be installed directly from the Obsidian UI.\\n\\n1. Open Obsidian's Settings\\n2. Click on **Community Plugins**\\n3. Click on **Browse** to open the Community Plugins Directory\\n4. Search for `AI Research Assistant`\\n5. Click on the plugin to open its page\\n6. Click **Install**\\n7. Return to the **Community Plugins** settings in Obsidian\\n8. Toggle the switch to enable `AI Research Assistant`\\n9. Configure `AI Research Assistant` in Obsidian's Settings\\n10. Enter your OpenAI API Key in the plugin's settings\\n11. Click the **Save API Key** button to save your API Key to Obsidian's secure storage\\n12. Click on the `AI Research Assistant` icon in the left sidebar to open the plugin and start chatting\\n\\n### Other Installation Methods\\n\\nYou can also [install it manually](#manual-installation) or [install it via the Beta Reviewer's Auto-update Tool (BRAT)](#brat-installation), though the official Community Plugin Directory is the recommended installation method.\\n\\n### BRAT Installation\\n\\n1. Install [BRAT](https://github.com/TfTHacker/obsidian42-brat)\\n2. Click the `Add Beta Plugin` button in BRAT's settings\\n3. Enter this repository's URL: `https://github.com/InterwebAlchemy/obsidian-ai-research-assistant`\\n4. Enable the plugin in Obsidian's Community Plugins settings\\n5. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n### Manual Installation\\n\\n1. Download the latest\\n   [release](https://github.com/InterwebAlchemy/obsidian-ai-research-assistant/releases)\\n2. Unzip the release into your Obsidian vault's `plugins` folder\\n3. Enable the plugin in Obsidian's Community Plugins settings\\n4. Configure `AI Research Assistant` in Obsidian's Settings\\n\\n## Currently Supported Models\\n\\n- [OpenAI](https://platform.openai.com/docs/models)\\n  - [`gpt-4`](https://platform.openai.com/docs/models/gpt-4)\\n  - [`gpt-3.5-turbo`](https://platform.openai.com/docs/models/gpt-3-5)\\n\\n### Upcoming Model Support\\n\\n- [OpenAI Legacy Completions API](https://platform.openai.com/docs/deprecations/instructgpt-models)\\n  - `gpt-3.5-turbo-instruct`\\n  - **Note**: This plugin was originally built when only the completions API was available and was built to support the `text-davinci-003` model, but with recent changes in the OpenAI API, it is now recommended to use the `gpt-3.5-turbo` model instead and `text-daivinci-003` is being deprecated, so that functionality has been temporarily disabled in this plugin.\\n- Llama2\\n- Anthropic Claude\\n\\n## Naming Conventions\\n\\nThis plugin uses the following naming conventions to refer to different pieces of a conversation and\\nmake sure that it is always clear what is being referred to:\\n\\n**Note**: This plugin makes a distinction between a `Prompt` and a `Preamble` because it can be used\\nto generate prompts for models without a Preamble and in that case it is harder to distinguish\\nbetween what you might be editing when you click on an `Edit Prompt` button.\\n\\n- **Conversation**: This is an ongoing exchange of messages between the Human and the AI.\\n- **Preamble**: This is the initial instructions that a language model recieves. It is usually a\\n  short description of the topic of the conversation, and is used to provide context on how the\\n  model should behave, what the models knows, and how it should respond.\\n  - **Notable Examples**:\\n    - [ChatGPT](https://twitter.com/goodside/status/1598253337400717313)\\n    - [Perplexity AI](https://twitter.com/jmilldotdev/status/1600624362394091523)\\n    - [Bing Chat](https://twitter.com/kliu128/status/1623472922374574080)\\n- **Prompt**: Prompts are the questions that the model is asked to answer. They are usually a single\\n  sentence or a short paragraph.\\n  - **Notable Examples**:\\n    - [ShareGPT](https://sharegpt.com/)\\n    - [OpenAI Examples](https://platform.openai.com/examples/)\\n- **Context**: Context is the memory that the model uses to generate its response. It usually\\n  consists of the Preamble and some previous messages (or summaries them), and older messages are\\n  eventually replaced by newer ones as the conversation progresses and tokens becomes more limited.\\n- **Prefixes**: Prefixes are prepended to a Prompt and are generally used to include a Start Word\\n  that identifies the start of a Prompt and a Handle that identifies the speaker.\\n- **Suffixes**: Suffixes are appended to a Prompt and are generally used to include a Stop Word that\\n  identifies the end of a Prompt.\\n- **Handles**: Handles are short identifiers used to differentiate between speakers in the Context.\\n  By default this plugin uses `You:` to represent the Human and `AI:` to represent the AI model. You\\n  can change these prefixes in the AI Research Assistant settings.\\n- **Memories**: When [Memories](#memories) are enabled, the Context is generated from the Preamble\\n  and a defined number of previous messages. If the experimental Memory Manager is enabled, you can\\n  edit the Context in real time for each Prompt by defining memory staties for previous messages.\\n\\n## Memories\\n\\nMemories give your Conversation context and allow the language model to use what's already been said\\nto inform future responses.\\n\\nThere are four different [Memory States](#memory-states), and each message can have one of them:\\n\\nYou can manage the Memories of a Conversation in real time by clicking on the Memory State button\\nnext to each message bubble and choosing a new state for that memory.\\n\\n### Memory States\\n\\n- **Default**: By default, messages are **somtimes** remembered (in reverse chronological order),\\n  the most recent Memories are most likely to be remembered and older Memories are the first to be\\n  excluded frm the Conversation's Context. These Memories follow a basic First In First Out (FIFO)\\n  queue.\\n- **Core Memory**: Core Memories are **always** included in the Conversation's Context. They are\\n  usually the most important messages in the conversation and are used to provide consistent Context\\n  for the model. They do not count towards the maximum number of memories for a Conversation\\n  configured in the plugin's settings.\\n- **Remembered**: Remembered Memories **often** included in the Conversation's Context, until your\\n  Conversation reaches the maximum number of memories configured in the plugin's settings. They are\\n  usually more important messages, but not as important as Core Memories. They do count towards the\\n  maximum number of memories, and are accessed in reverse chronological order. If the maximum number\\n  of memories is reached, the oldest Remembered Memories will not be included in the Context.\\n- **Forgotten**: Forgotten Memories are **never** included in the Conversation's Context. You can\\n  forget a Memory at any time during the Conversation, and also restore Forgotten Memories to\\n  another Memory State.\\n---\"]\n",
      "DEBUG:root:        create_time: 1693281218.583268\n",
      "DEBUG:root:        end_turn: None\n",
      "DEBUG:root:        id: aaa2bd91-80e5-456c-8f73-6f0cf6adcd15\n",
      "DEBUG:root:        metadata:\n",
      "DEBUG:root:          finish_details: None\n",
      "DEBUG:root:          is_complete: None\n",
      "DEBUG:root:          message_type: None\n",
      "DEBUG:root:          model_slug: None\n",
      "DEBUG:root:          parent_id: None\n",
      "DEBUG:root:          timestamp_: absolute\n",
      "DEBUG:root:        recipient: all\n",
      "DEBUG:root:        status: finished_successfully\n",
      "DEBUG:root:        update_time: None\n",
      "DEBUG:root:        weight: 1.0\n",
      "DEBUG:root:      parent: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:root:  moderation_results: []\n",
      "DEBUG:root:  plugin_ids: None\n",
      "DEBUG:root:  title: Import ChatGPT JSON Format\n",
      "DEBUG:root:  update_time: 1693281747.0\n",
      "DEBUG:root:================================================================================\n"
     ]
    }
   ],
   "source": [
    "from tapestrymd.parse_conversation_tree import find_shortest_and_longest_paths\n",
    "from tapestrymd.types import Conversation, debug_log_dict\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "conversation_obj_list = []\n",
    "for conversation_data in data[:1]:\n",
    "    print(conversation_data)\n",
    "    conversation_deepcopy = copy.deepcopy(conversation_data)\n",
    "    conversation_obj = Conversation.from_dict(conversation_deepcopy)\n",
    "    logging.debug(f\"{'=' * 80}\")\n",
    "    conversation_dict = conversation_obj.to_dict()\n",
    "    debug_log_dict(conversation_dict, 'Conversation Object to Dict')\n",
    "    logging.debug(f\"{'=' * 80}\")\n",
    "    conversation_obj_list.append(conversation_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:tapestrymd.parse_conversation_tree:Building traversal tree for conversation 930bb6fd-00e1-4abc-8ab9-5f77c39bbb28\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Initializing new traversal tree for conversation 930bb6fd-00e1-4abc-8ab9-5f77c39bbb28\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Found root node on init MessageRecord(id='aaa13816-46a6-4b26-ac95-dc204c5220ef', parent=None, children=['55308928-5076-4c81-bbfd-4bc504b237eb'], message=None)\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Root message record: MessageRecord(id='aaa13816-46a6-4b26-ac95-dc204c5220ef', parent=None, children=['55308928-5076-4c81-bbfd-4bc504b237eb'], message=None)\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Traversal tree initalized with root node: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Traversal tree initalized with current node: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Traversal tree root node: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Traversal tree current node: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Adding message record to traversal tree: MessageRecord(id='55308928-5076-4c81-bbfd-4bc504b237eb', parent='aaa13816-46a6-4b26-ac95-dc204c5220ef', children=['aaa2bd91-80e5-456c-8f73-6f0cf6adcd15'], message=ChatMessage(id='55308928-5076-4c81-bbfd-4bc504b237eb', create_time=None, update_time=None, author=Author(role=<RoleType.SYSTEM: 'system'>, name=None, metadata={}), content=TextContent(content_type=<ContentType.TEXT: 'text'>, metadata={}, parts=['']), status='finished_successfully', end_turn=True, weight=0.0, metadata={}, recipient='all'))\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Message record 55308928-5076-4c81-bbfd-4bc504b237eb has parent aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Adding child 55308928-5076-4c81-bbfd-4bc504b237eb of type <class 'tapestrymd.types.MessageRecord'> to parent aaa13816-46a6-4b26-ac95-dc204c5220ef of type <class 'str'>\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Finding node aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Node aaa13816-46a6-4b26-ac95-dc204c5220ef not found\n",
      "INFO:tapestrymd.parse_conversation_tree:Parent node aaa13816-46a6-4b26-ac95-dc204c5220ef not found in tree, but found in mapping.\n",
      "INFO:tapestrymd.parse_conversation_tree:Parent node aaa13816-46a6-4b26-ac95-dc204c5220ef has no parent in mapping. Checking if parent is root node.\n",
      "INFO:tapestrymd.parse_conversation_tree:Parent node aaa13816-46a6-4b26-ac95-dc204c5220ef is root node. Using root node as parent.\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Child node: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Parent node: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "DEBUG:tapestrymd.parse_conversation_tree:Child has parent: aaa13816-46a6-4b26-ac95-dc204c5220ef\n",
      "ERROR:tapestrymd.parse_conversation_tree:Parent does not have child: 55308928-5076-4c81-bbfd-4bc504b237eb\n",
      "WARNING:tapestrymd.parse_conversation_tree:Child node 55308928-5076-4c81-bbfd-4bc504b237eb found in conversation mapping. Hacking it in.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\blake\\workspace\\chatgpt-as-obsidian-vault\\notebooks\\testing_conversation_traversal_tree.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/blake/workspace/chatgpt-as-obsidian-vault/notebooks/testing_conversation_traversal_tree.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m conversation_obj \u001b[39min\u001b[39;00m conversation_obj_list:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/blake/workspace/chatgpt-as-obsidian-vault/notebooks/testing_conversation_traversal_tree.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     shortest_path, longest_path \u001b[39m=\u001b[39m find_shortest_and_longest_paths(conversation_obj)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/blake/workspace/chatgpt-as-obsidian-vault/notebooks/testing_conversation_traversal_tree.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversation ID|Title: \u001b[39m\u001b[39m{\u001b[39;00mconversation_obj\u001b[39m.\u001b[39mid\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mconversation_obj\u001b[39m.\u001b[39mtitle\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/blake/workspace/chatgpt-as-obsidian-vault/notebooks/testing_conversation_traversal_tree.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShortest path: \u001b[39m\u001b[39m{\u001b[39;00mshortest_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\workspace\\chatgpt-as-obsidian-vault\\tapestrymd\\parse_conversation_tree.py:222\u001b[0m, in \u001b[0;36mfind_shortest_and_longest_paths\u001b[1;34m(conversation)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_shortest_and_longest_paths\u001b[39m(conversation: Conversation) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m], \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]]:\n\u001b[1;32m--> 222\u001b[0m     traversal_tree \u001b[39m=\u001b[39m build_conversation_traversal_tree(conversation)\n\u001b[0;32m    223\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraversing conversation \u001b[39m\u001b[39m{\u001b[39;00mconversation\u001b[39m.\u001b[39mconversation_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    224\u001b[0m     paths \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\workspace\\chatgpt-as-obsidian-vault\\tapestrymd\\parse_conversation_tree.py:214\u001b[0m, in \u001b[0;36mbuild_conversation_traversal_tree\u001b[1;34m(conversation)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m message_record\u001b[39m.\u001b[39mparent:\n\u001b[0;32m    213\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMessage record \u001b[39m\u001b[39m{\u001b[39;00mmessage_record\u001b[39m.\u001b[39mid\u001b[39m}\u001b[39;00m\u001b[39m has parent \u001b[39m\u001b[39m{\u001b[39;00mmessage_record\u001b[39m.\u001b[39mparent\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 214\u001b[0m         traversal_tree\u001b[39m.\u001b[39;49madd_child(message_record\u001b[39m.\u001b[39;49mparent, message_record)\n\u001b[0;32m    216\u001b[0m \u001b[39mreturn\u001b[39;00m traversal_tree\n",
      "File \u001b[1;32m~\\workspace\\chatgpt-as-obsidian-vault\\tapestrymd\\parse_conversation_tree.py:170\u001b[0m, in \u001b[0;36mConversationTraversalTree.add_child\u001b[1;34m(self, parent, child)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mChild node \u001b[39m\u001b[39m{\u001b[39;00mchild_node\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m found in conversation mapping. Hacking it in.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m     parent_node\u001b[39m.\u001b[39;49mchildren \u001b[39m=\u001b[39m parent_node\u001b[39m.\u001b[39mchildren \u001b[39m+\u001b[39m (child_node,)\n",
      "File \u001b[1;32mc:\\Users\\blake\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tapestrymd--Vi_840p-py3.11\\Lib\\site-packages\\anytree\\node\\nodemixin.py:252\u001b[0m, in \u001b[0;36mNodeMixin.children\u001b[1;34m(self, children)\u001b[0m\n\u001b[0;32m    250\u001b[0m         child\u001b[39m.\u001b[39mparent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_attach_children(children)\n\u001b[1;32m--> 252\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(children)\n\u001b[0;32m    253\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren \u001b[39m=\u001b[39m old_children\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for conversation_obj in conversation_obj_list:\n",
    "    shortest_path, longest_path = find_shortest_and_longest_paths(conversation_obj)\n",
    "    print(f\"Conversation ID|Title: {conversation_obj.id}|{conversation_obj.title}\")\n",
    "    print(f\"Shortest path: {shortest_path}\")\n",
    "    print(f\"Longest path: {longest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt-as-obsidian-vault--Vi_840p-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
